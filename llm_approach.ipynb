{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94088def",
   "metadata": {},
   "source": [
    "## LLM Approach: Scientific Documentation\n",
    "\n",
    "### High-level overview\n",
    "- **Goal**: Detect clusters of related bot accounts based on writing style (stylometry), optionally fusing with embedding-based similarity and labeling clusters with an LLM.\n",
    "- **Pipeline**: data loading → normalization and chunking → TF‑IDF char n‑gram vectorization → randomized “impostors” AV similarity → optional embedding-based style/topical similarity → late fusion → graph thresholding + Louvain community detection → optional topic-aware merges → optional LLM-based labeling.\n",
    "\n",
    "### Imports and dependencies\n",
    "- Core: `os`, `re`, `random`, `itertools`, `typing`.\n",
    "- Scientific: `numpy`, `pandas`.\n",
    "- Stylometry: `sklearn.feature_extraction.text.TfidfVectorizer` (character n‑grams), `sklearn.metrics.pairwise.cosine_similarity`.\n",
    "- Graph clustering: `networkx` with `louvain_communities` and `modularity`.\n",
    "- LLM stack (optional): `langchain_openai.OpenAIEmbeddings`, `langchain_openai.ChatOpenAI`, `langchain_core` prompt + JSON parser, `pydantic` for typed JSON.\n",
    "\n",
    "Rationale: Character n‑grams capture orthography, punctuation, casing, and function words, which are stable stylistic markers; Louvain finds dense communities in a similarity graph.\n",
    "\n",
    "### Data loading and account construction\n",
    "- Loads `OPENAI_API_KEY` via `dotenv`; if missing, embedding/LLM steps are skipped.\n",
    "- Reads `./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_sample_enriched.csv` into `df`.\n",
    "- `get_tweets_from_user(user_id)`: filters non-retweets and returns a list of `tweet_text` strings.\n",
    "- Builds `accounts: Dict[str, List[str]]` mapping synthetic IDs (`acc_A` …) to tweets from selected user IDs.\n",
    "\n",
    "Purpose: Define a small cohort of accounts with their text to run the pipeline end-to-end.\n",
    "\n",
    "### Normalization and chunking for stylometry\n",
    "- Regex normalization removes URLs and `@mentions`, lowercases, optional hashtag symbol stripping (keeps tokens), canonicalizes HTML `&amp;`, and condenses whitespace.\n",
    "- `unique_norm_texts`: normalizes and deduplicates texts.\n",
    "- `make_char_chunks(texts, target_chars≈8000, max_chunks=12)`: concatenates normalized tweets into several long chunks per account.\n",
    "  - Longer chunks stabilize character n‑gram distributions and reduce sparsity/noise.\n",
    "\n",
    "Scientific note: Character n‑grams are less topic-sensitive than word features and encode micro-level style (punctuation, casing, function words, emoji/hashtag usage).\n",
    "\n",
    "### Global TF‑IDF vectorization\n",
    "- `fit_vectorizer(acc_texts, ngram_range=(3,5), max_features=200k, sublinear_tf=True)`: builds a single global vocabulary across all chunks.\n",
    "  - Auto-adjusts `min_df`/`max_df` to be valid given the number of documents (chunks).\n",
    "  - Returns: fitted `TfidfVectorizer`, `acc_vecs` (per-account list of sparse vectors), `chunks_by_acc`, and `n_features`.\n",
    "\n",
    "Why global vocab: Ensures comparable features and global IDF statistics across accounts.\n",
    "\n",
    "### Randomized impostors AV similarity\n",
    "- Utility functions:\n",
    "  - `_rand_mask`: random feature subset mask of size `feat_frac * n_features`.\n",
    "  - `_pick_chunk`: random chunk vector for an account.\n",
    "  - `_cos_masked`: cosine similarity restricted to the masked features.\n",
    "- `impostors_score(A_chunks, B_chunks, background, n_features, n_trials, feat_frac, bg_per_trial)`:\n",
    "  - For each trial: sample features and one chunk for A and B, compute `s(A,B)`; sample `bg_per_trial` impostor accounts, compute max `s(A, impostor)`; count a win if `s(A,B)` exceeds that max.\n",
    "  - Score ∈ [0,1] = wins / n_trials.\n",
    "- `symmetric_impostors`: averages A→B and B→A.\n",
    "- `build_similarity_matrix_AV`: full pairwise symmetric matrix for all accounts.\n",
    "\n",
    "Scientific rationale: Randomized subspaces plus adversarial impostors yield a robust verification signal less biased by any fixed feature set or topical alignment.\n",
    "\n",
    "Key hyperparameters: `n_trials` (stability vs compute), `feat_frac` (feature bagging strength), `bg_per_trial` (difficulty of impostor competition).\n",
    "\n",
    "### Embedding-based style and topic representations\n",
    "- Style-focused embeddings:\n",
    "  - `style_prompt_prefix`: conditions the text to focus the embedder on stylistic aspects, not content topics.\n",
    "  - `build_account_embeddings`: embed all chunks per account (with style prompt), mean-pool, L2-normalize to unit length.\n",
    "  - `cosine_matrix_from_embeddings`: builds a [0,1]-mapped cosine matrix among accounts.\n",
    "- Topic-focused embeddings:\n",
    "  - `_cluster_topic_text`: concatenates normalized tweets of members of a cluster (no style prompt; we want topical semantics).\n",
    "  - `build_cluster_topic_embeddings`: one embedding per cluster; vectors are L2-normalized.\n",
    "- `cosine_01(a,b) = (cos(a,b) + 1)/2`: maps cosine from [-1,1] to [0,1].\n",
    "\n",
    "Separation of concerns: style embeddings aim to capture writing habits; topic embeddings capture semantic similarity for post-hoc merges.\n",
    "\n",
    "### Late fusion and community detection\n",
    "- `fuse_scores(S_av, S_embed, alpha)`: linear fusion `S_fused = α S_av + (1−α) S_embed`.\n",
    "- `louvain_partition(S, ids, tau, resolution)`: threshold `S ≥ τ` to construct a weighted graph, run Louvain, compute modularity `Q`.\n",
    "- `sweep_tau`: grid over τ to maximize `Q`; returns best τ, modularity, and clusters.\n",
    "\n",
    "Scientific points: Threshold controls graph density and separability. Louvain finds dense, modular communities; fusion leverages complementary strengths of stylometry and embeddings.\n",
    "\n",
    "### LLM-based cluster labeling with typed JSON\n",
    "- Pydantic `ClusterLabel` schema: `cluster_name`, `style_signature` (5–10 concise markers), `cohesion_summary`, optional `notable_outliers`.\n",
    "- `LABEL_PROMPT`: instructs the LLM to focus strictly on style and return STRICT JSON.\n",
    "- `llm_label_clusters`: builds short style-focused samples for members, queries `ChatOpenAI`, parses via `JsonOutputParser` into `ClusterLabel` instances.\n",
    "\n",
    "Benefit: Structured, machine-parseable labels that articulate stylistic commonalities, not topical content.\n",
    "\n",
    "### Topic-aware cluster merging\n",
    "- `merge_clusters_by_topic`:\n",
    "  - Build topic embeddings per cluster.\n",
    "  - Compute cluster–cluster cosine similarities; merge with union-find when similarity ≥ `topic_tau`.\n",
    "  - Reindex merged groups to consecutive IDs.\n",
    "\n",
    "Purpose: Fix over-fragmentation due to graph thresholding by merging topically similar clusters that were stylistically coherent but split across τ boundaries.\n",
    "\n",
    "### Pairwise diagnostics\n",
    "- `pair_score` and `print_pair_diagnostics`: quick inspection of select account pairs to compare `S_av`, `S_embed`, and `S_fused`.\n",
    "\n",
    "Use case: Sanity checks and interpretability of the fusion behavior on hand-picked examples.\n",
    "\n",
    "### Orchestration: end-to-end run\n",
    "- Set seed for reproducibility.\n",
    "- Validate that `accounts` is populated.\n",
    "- A) Vectorize and build stylometric chunk vectors (`fit_vectorizer`).\n",
    "- B) Compute AV similarity matrix `S_av` (`build_similarity_matrix_AV`).\n",
    "- C) If API key available: build style embeddings per account → `S_embed`; else, proceed with AV only.\n",
    "- D) Fuse (`alpha ≈ 0.6`), sweep τ for Louvain, report best `τ` and modularity `Q` with discovered communities.\n",
    "- E) If embeddings available: merge clusters by topic (`topic_tau` e.g., 0.70–0.80).\n",
    "- Optionally label merged clusters via LLM and print cluster name, style signature, and summary.\n",
    "\n",
    "### Practical notes\n",
    "- Char n‑grams: robust, topic-agnostic stylometry features for short, noisy social texts.\n",
    "- Impostors method: randomized subspaces and competition against many impostors calibrate a reliable [0,1] similarity.\n",
    "- Fusion: late fusion is simple and interpretable; tune `alpha` with validation or stability analyses.\n",
    "- Threshold τ: higher τ → tighter, smaller communities (precision↑, recall↓). Modularity sweep is a practical heuristic.\n",
    "- Randomness: trials, feature masks, chunk sampling introduce variance; seeds improve reproducibility; more trials improve stability at higher cost.\n",
    "- Data sufficiency: aim for ≥2–3 chunks per account for stable stylometry.\n",
    "- LLM labeling: ensure samples are style-focused; validate JSON parsing and outputs.\n",
    "- Complexity: AV similarity dominates runtime; consider reducing `n_trials`, `bg_per_trial`, or cohort size for large datasets.\n",
    "\n",
    "### Possible extension\n",
    "- Tune chunking (`target_chars`, `max_chunks`) for stability vs data availability.\n",
    "- Adjust `(ngram_range, feat_frac, bg_per_trial, n_trials)` for compute–stability trade-offs.\n",
    "- Experiment with different embedding models/providers and prompts for style/topic separation.\n",
    "- Try alternative clustering (Leiden, spectral) and graph construction strategies.\n",
    "- Calibrate `alpha` and `tau` using held-out linked accounts or stability criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512b5a2",
   "metadata": {},
   "source": [
    "### Imports and dependencies\n",
    "- Standard library, scientific stack, TF‑IDF for char n‑grams, cosine similarity.\n",
    "- Graph clustering via NetworkX Louvain and modularity.\n",
    "- Optional LLM stack: LangChain `OpenAIEmbeddings`, `ChatOpenAI`, prompt templates, JSON parser; Pydantic for schema-validated outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "10d05f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities           \n",
    "from networkx.algorithms.community.quality import modularity             \n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI              \n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaba2a6",
   "metadata": {},
   "source": [
    "### Data loading and account construction\n",
    "- Load environment and API key.\n",
    "- Read enriched tweets CSV.\n",
    "- Helper `get_tweets_from_user` filters original tweets.\n",
    "- Build `accounts` mapping from synthetic IDs to tweet lists for chosen user IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f617ba5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    print(\"[WARN] OPENAI_API_KEY not set. Embeddings & LLM labeling will be disabled.\")\n",
    "\n",
    "df = pd.read_csv(\"./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_sample_enriched.csv\")\n",
    "\n",
    "def get_tweets_from_user(user_id: int) -> List[str]:\n",
    "    return df.loc[(df[\"user_id\"] == user_id) & (df[\"is_retweet\"] == False), \"tweet_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "accounts: Dict[str, List[str]] = {\n",
    "    \"acc_A\": get_tweets_from_user(21479334), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_B\": get_tweets_from_user(1544624887), # starwars\n",
    "    \"acc_C\": get_tweets_from_user(1585571839), # starwars\n",
    "    \"acc_D\": get_tweets_from_user(21674676), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_E\": get_tweets_from_user(1546564405), # starwars\n",
    "    \"acc_F\": get_tweets_from_user(1587209611), # starwars\n",
    "    \"acc_G\": get_tweets_from_user(1552496288), # starwars\n",
    "    \"acc_H\": get_tweets_from_user(22728442), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_I\": get_tweets_from_user(22759829), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_J\": get_tweets_from_user(22578323), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_K\": get_tweets_from_user(74793689), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_L\": get_tweets_from_user(38095383), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_M\": get_tweets_from_user(1557300746), # starwars\n",
    "    \"acc_N\": get_tweets_from_user(1559362050), # starwars\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113bd02",
   "metadata": {},
   "source": [
    "### Normalization and chunking\n",
    "- Normalize tweets: lowercase, remove URLs and `@mentions`, strip `#` (keeping tokens), convert `&amp;`, and condense whitespace.\n",
    "- Deduplicate normalized texts.\n",
    "- Concatenate into character-length chunks (default ≈8k chars, up to 12) to stabilize stylometric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dfb88b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "WS_RE = re.compile(r'\\s+')\n",
    "\n",
    "def normalize_tweet(t: str, keep_hashtags: bool = True) -> str:\n",
    "    t = t.lower()\n",
    "    t = URL_RE.sub(' ', t)\n",
    "    t = MENTION_RE.sub(' ', t)\n",
    "    if keep_hashtags:\n",
    "        t = t.replace('#', '')\n",
    "    t = t.replace('&amp;', '&')\n",
    "    t = WS_RE.sub(' ', t).strip()\n",
    "    return t\n",
    "\n",
    "def unique_norm_texts(texts: List[str]) -> List[str]:\n",
    "    seen, out = set(), []\n",
    "    for s in texts:\n",
    "        n = normalize_tweet(s)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); out.append(n)\n",
    "    return out\n",
    "\n",
    "def make_char_chunks(texts: List[str], target_chars: int = 8000, max_chunks: int = 12) -> List[str]:\n",
    "    \"\"\"\n",
    "    Larger target_chars -> more stable AV (aim for >= 2–3 chunks per account).\n",
    "    \"\"\"\n",
    "    msgs = unique_norm_texts(texts)\n",
    "    if not msgs:\n",
    "        return []\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for m in msgs:\n",
    "        cur.append(m); cur_len += len(m) + 1\n",
    "        if cur_len >= target_chars:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_len = [], 0\n",
    "            if len(chunks) >= max_chunks:\n",
    "                break\n",
    "    if cur and len(chunks) < max_chunks:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks if chunks else [\" \".join(msgs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803beeac",
   "metadata": {},
   "source": [
    "### Global TF‑IDF stylometry\n",
    "- Build a single character n‑gram TF‑IDF vocabulary across all chunks (`(3,5)`-grams, `sublinear_tf=True`).\n",
    "- Auto-adjust `min_df`/`max_df` to be valid for the observed number of documents (chunks).\n",
    "- Return the vectorizer, per-account chunk vectors, chunk texts, and feature count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0f771dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(acc_texts: Dict[str, List[str]], ngram_range: Tuple[int, int] = (3, 5), max_features: int = 200_000, min_df=None, max_df=None, sublinear_tf: bool = True):\n",
    "    \"\"\"\n",
    "    Build one global TF-IDF vocab across all account chunks (robust for AV).\n",
    "    Auto-adjust min_df/max_df to avoid conflicts (max_df_docs >= min_df_docs).\n",
    "    \"\"\"\n",
    "    docs, ids, chunks_by_acc = [], [], {}\n",
    "    per_acc = {}\n",
    "    for aid, texts in acc_texts.items():\n",
    "        chs = make_char_chunks(texts, target_chars=8000, max_chunks=12)\n",
    "        chunks_by_acc[aid] = chs[:]\n",
    "        per_acc[aid] = len(chs)\n",
    "        for ch in chs:\n",
    "            docs.append(ch); ids.append(aid)\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < 2:\n",
    "        raise ValueError(\"Too few chunks. Add data or reduce target_chars.\")\n",
    "\n",
    "    umin, umax = min_df, max_df\n",
    "    if min_df is None: min_df = max(1, int(0.01 * n_docs))\n",
    "    if max_df is None: max_df = 0.90\n",
    "    def _to_docs(val, n): return int(val * n) if isinstance(val, float) else int(val)\n",
    "    max_docs = _to_docs(max_df, n_docs)\n",
    "    min_docs = _to_docs(min_df, n_docs) if isinstance(min_df, int) else _to_docs(min_df, n_docs)\n",
    "    if max_docs < min_docs:\n",
    "        min_docs = max(1, max_docs)\n",
    "        min_df = min_docs if not isinstance(umin, float) else max(umin, min_docs / n_docs)\n",
    "    max_docs = _to_docs(max_df, n_docs)\n",
    "    min_docs = _to_docs(min_df, n_docs) if isinstance(min_df, int) else _to_docs(min_df, n_docs)\n",
    "    if max_docs < min_docs:\n",
    "        max_df = min(0.99, (min_docs + 1) / n_docs) if isinstance(max_df, float) else (min_docs + 1)\n",
    "\n",
    "    print(f\"[fit_vectorizer] n_docs={n_docs} per_account={per_acc} min_df={min_df} max_df={max_df}\")\n",
    "\n",
    "    vec = TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=ngram_range,\n",
    "        min_df=min_df, max_df=max_df, max_features=max_features,\n",
    "        norm=\"l2\", sublinear_tf=sublinear_tf\n",
    "    )\n",
    "    \n",
    "    X = vec.fit_transform(docs)\n",
    "\n",
    "    acc_vecs = {}\n",
    "    for i, aid in enumerate(ids):\n",
    "        acc_vecs.setdefault(aid, []).append(X[i])\n",
    "    n_features = len(vec.get_feature_names_out())\n",
    "    return vec, acc_vecs, chunks_by_acc, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261db26e",
   "metadata": {},
   "source": [
    "### Impostors AV similarity and randomized subspaces\n",
    "- Randomly sample feature subsets (`feat_frac`) and one chunk per account per trial.\n",
    "- Compute masked cosine `s(A,B)` vs the maximum `s(A, impostor)` over sampled background accounts.\n",
    "- Score is the fraction of trials where A beats impostors when paired with B; symmetrize A↔B.\n",
    "- Build the full similarity matrix `S_av` across all accounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "481c2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def _rand_mask(n_features: int, keep_frac: float) -> np.ndarray:\n",
    "    k = max(1, int(n_features * keep_frac))\n",
    "    mask = np.zeros(n_features, dtype=bool)\n",
    "    idx = np.random.choice(n_features, size=k, replace=False)\n",
    "    mask[idx] = True\n",
    "    return mask\n",
    "\n",
    "def _cos_masked(a, b, mask: np.ndarray) -> float:\n",
    "    return float(cosine_similarity(a[:, mask], b[:, mask])[0, 0])\n",
    "\n",
    "def _pick_chunk(vecs: List) -> np.ndarray:\n",
    "    if not vecs: raise ValueError(\"Account has no chunk vectors.\")\n",
    "    return vecs[np.random.randint(len(vecs))]\n",
    "\n",
    "def impostors_score(A_chunks: List, B_chunks: List, background: List[List],\n",
    "                    n_features: int, n_trials=1200, feat_frac=0.45, bg_per_trial=70) -> float:\n",
    "    if not background: return 0.0\n",
    "    wins, poolN = 0, len(background)\n",
    "    k = min(bg_per_trial, poolN)\n",
    "    for _ in range(n_trials):\n",
    "        mask = _rand_mask(n_features, feat_frac)\n",
    "        vA, vB = _pick_chunk(A_chunks), _pick_chunk(B_chunks)\n",
    "        sAB = _cos_masked(vA, vB, mask)\n",
    "        idx = np.random.choice(poolN, size=k, replace=False)\n",
    "        sAI = [_cos_masked(vA, _pick_chunk(background[j]), mask) for j in idx]\n",
    "        if sAB > max(sAI, default=-1.0):\n",
    "            wins += 1\n",
    "    return wins / n_trials\n",
    "\n",
    "def symmetric_impostors(A_chunks, B_chunks, background, n_features, n_trials=1200, feat_frac=0.45, bg_per_trial=70) -> float:\n",
    "    s1 = impostors_score(A_chunks, B_chunks, background, n_features, n_trials, feat_frac, bg_per_trial)\n",
    "    s2 = impostors_score(B_chunks, A_chunks, background, n_features, n_trials, feat_frac, bg_per_trial)\n",
    "    return 0.5 * (s1 + s2)\n",
    "\n",
    "def build_similarity_matrix_AV(ids: List[str], acc_vecs: Dict[str, List], n_features: int, n_trials=1200, feat_frac=0.45, bg_per_trial=70, seed=42):\n",
    "    np.random.seed(seed)\n",
    "    n = len(ids)\n",
    "    S = np.eye(n, dtype=float)\n",
    "    chunks = {i: acc_vecs[i] for i in ids}\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        Ai, Bj = ids[i], ids[j]\n",
    "        back = [chunks[k] for k in ids if k not in (Ai, Bj)]\n",
    "        s = symmetric_impostors(chunks[Ai], chunks[Bj], back, n_features, n_trials=n_trials, feat_frac=feat_frac, bg_per_trial=bg_per_trial)\n",
    "        S[i, j] = S[j, i] = s\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac49307",
   "metadata": {},
   "source": [
    "### Embeddings for style and topic\n",
    "- Style embeddings: prefix text with a style-only instruction, embed chunks, mean-pool, L2-normalize; derive `S_embed`.\n",
    "- Topic embeddings: concatenate normalized texts per cluster (no style prompt), embed and L2-normalize; used to compare and merge clusters by topic.\n",
    "- Cosine values are mapped to [0,1] via `(cos + 1)/2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d53fc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_prompt_prefix(text: str) -> str:\n",
    "    return (\n",
    "        \"Instruction: Create an embedding that captures WRITING STYLE only \"\n",
    "        \"(punctuation, casing, rhythm, emoji/hashtag usage, function words), not topical content.\\n\\n\"\n",
    "        \"TEXT:\\n\" + text\n",
    "    )\n",
    "\n",
    "def build_account_embeddings(chunks_by_acc: Dict[str, List[str]], embedder) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Per-account L2-normalized mean of chunk embeddings (STYLE-focused).\"\"\"\n",
    "    acc_vec = {}\n",
    "    for aid, chunks in chunks_by_acc.items():\n",
    "        if not chunks:\n",
    "            acc_vec[aid] = None\n",
    "            continue\n",
    "        docs = [style_prompt_prefix(ch) for ch in chunks]\n",
    "        embs = embedder.embed_documents(docs)      # LangChain OpenAIEmbeddings # :contentReference[oaicite:10]{index=10}\n",
    "        M = np.array(embs, dtype=np.float32)\n",
    "        mu = M.mean(axis=0); mu /= (np.linalg.norm(mu) + 1e-12)\n",
    "        acc_vec[aid] = mu\n",
    "    return acc_vec\n",
    "\n",
    "def cosine_matrix_from_embeddings(ids: List[str], acc_emb: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    mat = np.vstack([acc_emb[i] for i in ids])\n",
    "    C = np.clip(mat @ mat.T, -1.0, 1.0)\n",
    "    return 0.5 * (C + 1.0)\n",
    "\n",
    "def _cluster_topic_text(cluster_members: List[str], accounts_texts: Dict[str, List[str]], max_chars_per_acc=1200) -> str:\n",
    "    \"\"\"\n",
    "    Build a short, topic-focused sample for a whole cluster by concatenating\n",
    "    (normalized) tweets of its members. We intentionally DO NOT use the style\n",
    "    prompt here; we want topical semantics.\n",
    "    \"\"\"\n",
    "    out, total = [], 0\n",
    "    for aid in cluster_members:\n",
    "        for t in accounts_texts[aid]:\n",
    "            t = normalize_tweet(t) \n",
    "            if total + len(t) + 1 > max_chars_per_acc * len(cluster_members):\n",
    "                break\n",
    "            out.append(t); total += len(t) + 1\n",
    "    return \"\\n\".join(out) if out else \"\"\n",
    "\n",
    "def build_cluster_topic_embeddings(clusters: Dict[int, List[str]], accounts_texts: Dict[str, List[str]], embedder) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    One embedding per cluster using raw (normalized) text concatenated.\n",
    "    This captures TOPIC similarity (not style).\n",
    "    \"\"\"\n",
    "    cl_vec = {}\n",
    "    docs = []\n",
    "    order = []\n",
    "    for cid, members in clusters.items():\n",
    "        txt = _cluster_topic_text(members, accounts_texts)\n",
    "        if not txt: continue\n",
    "        docs.append(txt); order.append(cid)\n",
    "    if not docs:\n",
    "        return {}\n",
    "    embs = embedder.embed_documents(docs)\n",
    "    for i, cid in enumerate(order):\n",
    "        v = np.array(embs[i], dtype=np.float32)\n",
    "        v /= (np.linalg.norm(v) + 1e-12)\n",
    "        cl_vec[cid] = v\n",
    "    return cl_vec\n",
    "\n",
    "def cosine_01(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    c = float(np.clip(np.dot(a, b), -1.0, 1.0))\n",
    "    return 0.5 * (c + 1.0)  # map [-1,1] -> [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07691ab3",
   "metadata": {},
   "source": [
    "### Fusion and Louvain community detection\n",
    "- Fuse stylometric AV and embedding similarities: `S_fused = α S_av + (1−α) S_embed`.\n",
    "- Build a thresholded weighted graph at `S ≥ τ` and run Louvain; compute modularity `Q`.\n",
    "- Sweep τ to find the partition with maximal modularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "49d131cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_scores(S_av: np.ndarray, S_embed: Optional[np.ndarray] = None, alpha: float = 0.7) -> np.ndarray:\n",
    "    if S_embed is None:\n",
    "        return S_av\n",
    "    return alpha * S_av + (1.0 - alpha) * S_embed\n",
    "\n",
    "def louvain_partition(S: np.ndarray, ids: List[str], tau: float = 0.65, resolution: float = 1.0):\n",
    "    \"\"\"\n",
    "    Threshold S at tau -> weighted graph -> Louvain communities -> modularity Q.\n",
    "    NetworkX louvain_communities & modularity are used.                      \n",
    "    \"\"\"\n",
    "    G = nx.Graph(); G.add_nodes_from(ids)\n",
    "    n = len(ids)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            w = float(S[i, j])\n",
    "            if w >= tau:\n",
    "                G.add_edge(ids[i], ids[j], weight=w)\n",
    "    if G.number_of_edges() == 0:\n",
    "        return {}, 0.0\n",
    "    comms = louvain_communities(G, weight=\"weight\", resolution=resolution, seed=42)\n",
    "    Q = modularity(G, comms, weight=\"weight\")\n",
    "    clusters = {i: sorted(list(c)) for i, c in enumerate(comms)}\n",
    "    return clusters, Q\n",
    "\n",
    "def sweep_tau(S: np.ndarray, ids: List[str], taus=np.linspace(0.56, 0.72, 9), resolution=1.0):\n",
    "    best = {\"tau\": None, \"Q\": -1, \"clusters\": None}\n",
    "    for tau in taus:\n",
    "        cs, Q = louvain_partition(S, ids, tau=tau, resolution=resolution)\n",
    "        if Q is not None and Q > best[\"Q\"]:\n",
    "            best = {\"tau\": tau, \"Q\": Q, \"clusters\": cs}\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214e5b2",
   "metadata": {},
   "source": [
    "### LLM-based cluster labeling (structured JSON)\n",
    "- Pydantic schema `ClusterLabel` defines the target JSON structure.\n",
    "- Prompt instructs the LLM to focus on stylistic markers only.\n",
    "- For each cluster: build short style-focused samples, call the LLM, and parse with `JsonOutputParser`.\n",
    "\n",
    "Outcome: machine-parseable labels with a concise name, style signature, and cohesion summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f7e05178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLabel(BaseModel):\n",
    "    cluster_name: str = Field(..., description=\"Short English name for the botnet cluster\")\n",
    "    style_signature: List[str] = Field(..., description=\"5–10 concise style markers shared by the cluster\")\n",
    "    cohesion_summary: str = Field(..., description=\"2–3 sentences explaining WHY these accounts belong together stylistically (NOT topics)\")\n",
    "    notable_outliers: List[str] = Field(default_factory=list, description=\"Optional: account IDs that partially fit or diverge\")\n",
    "\n",
    "def _short_style_sample(texts: List[str], max_chars: int = 1500) -> str:\n",
    "    buf, total = [], 0\n",
    "    for t in texts:\n",
    "        if not t: continue\n",
    "        if total + len(t) + 1 > max_chars: break\n",
    "        buf.append(t); total += len(t) + 1\n",
    "    return \"\\n\".join(buf) if buf else \"\"\n",
    "\n",
    "LABEL_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a forensic writing analyst. Label a cluster of bot accounts discovered via STYLOMETRIC similarity.\n",
    "Focus ONLY on WRITING STYLE (punctuation, casing, emoji/hashtag usage, function words, character n-grams, formatting quirks), NOT topics.\n",
    "\n",
    "Return STRICT JSON with this schema:\n",
    "{schema}\n",
    "\n",
    "For each member you receive:\n",
    "- account_id\n",
    "- a short style-focused sample (possibly truncated)\n",
    "\n",
    "Members:\n",
    "{members}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def llm_label_clusters(clusters: Dict[int, List[str]],\n",
    "                       accounts_texts: Dict[str, List[str]],\n",
    "                       llm_model: str = \"gpt-4o-mini\",\n",
    "                       temperature: float = 0.0) -> Dict[int, ClusterLabel]:\n",
    "    if not API_KEY:\n",
    "        print(\"[INFO] Skipping LLM labeling (no OPENAI_API_KEY).\")\n",
    "        return {}\n",
    "    llm = ChatOpenAI(model=llm_model, temperature=temperature)\n",
    "    parser = JsonOutputParser(pydantic_object=ClusterLabel)\n",
    "    results: Dict[int, ClusterLabel] = {}\n",
    "    for cid, members in clusters.items():\n",
    "        lines = []\n",
    "        for aid in members:\n",
    "            sample = _short_style_sample([normalize_tweet(t) for t in accounts_texts[aid]], max_chars=1400)\n",
    "            lines.append(f\"- {aid}:\\n\\\"\\\"\\\"\\n{sample}\\n\\\"\\\"\\\"\")\n",
    "        prompt = LABEL_PROMPT.format_messages(\n",
    "            schema=parser.get_format_instructions(),\n",
    "            members=\"\\n\".join(lines)\n",
    "        )\n",
    "        resp = llm.invoke(prompt)\n",
    "        out = parser.parse(resp.content)\n",
    "        if isinstance(out, dict):  \n",
    "            out = ClusterLabel(**out)\n",
    "        results[cid] = out\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910d9c1",
   "metadata": {},
   "source": [
    "### Topic-aware cluster merging\n",
    "- Build topic embeddings per cluster by concatenating normalized member texts (no style prompt).\n",
    "- Merge clusters via union-find when cluster–cluster cosine ≥ `topic_tau`.\n",
    "- Reindex merged groups to consecutive IDs.\n",
    "\n",
    "Purpose: correct over-fragmentation from the thresholded graph by merging topically coherent clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ad09395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters_by_topic(clusters: Dict[int, List[str]], accounts_texts: Dict[str, List[str]], embedder, topic_tau: float = 0.80, verbose: bool = True) -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Merge clusters if their TOPIC embeddings cosine ≥ topic_tau.\n",
    "    - Build one \"topic\" embedding per cluster (concatenate normalized texts).\n",
    "    - Compute cosine similarity among clusters.\n",
    "    - Union-Find merge for all pairs above threshold (transitive closure).\n",
    "    \"\"\"\n",
    "    if not clusters:\n",
    "        return clusters\n",
    "    # Build cluster-level topic embeddings\n",
    "    cl_emb = build_cluster_topic_embeddings(clusters, accounts_texts, embedder)\n",
    "    if len(cl_emb) <= 1:\n",
    "        return clusters\n",
    "\n",
    "    # Union-Find structure\n",
    "    parent = {cid: cid for cid in clusters}\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    def union(a, b):\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra != rb:\n",
    "            parent[rb] = ra\n",
    "\n",
    "    # Compare all pairs\n",
    "    cids = sorted(cl_emb.keys())\n",
    "    for i in range(len(cids)):\n",
    "        for j in range(i+1, len(cids)):\n",
    "            ci, cj = cids[i], cids[j]\n",
    "            s = cosine_01(cl_emb[ci], cl_emb[cj])   \n",
    "            if s >= topic_tau:\n",
    "                if verbose:\n",
    "                    print(f\"[MERGE-TOPIC] merging clusters {ci} and {cj} (topic_sim={s:.3f} >= {topic_tau:.2f})\")\n",
    "                union(ci, cj)\n",
    "\n",
    "    # Build merged mapping\n",
    "    groups = {}\n",
    "    for cid in clusters:\n",
    "        root = find(cid)\n",
    "        groups.setdefault(root, [])\n",
    "        groups[root].extend(clusters[cid])\n",
    "\n",
    "    # Reindex cluster ids 0..K-1\n",
    "    merged = {}\n",
    "    for new_id, (_, members) in enumerate(sorted(groups.items(), key=lambda kv: tuple(kv[1]))):\n",
    "        merged[new_id] = sorted(set(members))\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1416e2",
   "metadata": {},
   "source": [
    "### Pairwise diagnostics\n",
    "Compare specific account pairs across signals:\n",
    "- `S_av`: stylometric impostors similarity\n",
    "- `S_embed`: style-embedding cosine (if available)\n",
    "- `S_fused`: late-fused score\n",
    "\n",
    "Use for sanity checks and qualitative inspection of fusion behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8cc10a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_score(ids: List[str], S: np.ndarray, a: str, b: str) -> float:\n",
    "    idx = {aid: i for i, aid in enumerate(ids)}\n",
    "    return float(S[idx[a], idx[b]])\n",
    "\n",
    "def print_pair_diagnostics(ids: List[str], S_av: np.ndarray, S_embed: Optional[np.ndarray], S_fused: np.ndarray,\n",
    "                           pairs: List[Tuple[str, str]]):\n",
    "    for a, b in pairs:\n",
    "        sav = pair_score(ids, S_av, a, b)\n",
    "        se = pair_score(ids, S_embed, a, b) if S_embed is not None else None\n",
    "        sf = pair_score(ids, S_fused, a, b)\n",
    "        if se is None:\n",
    "            print(f\"[PAIR] {a} vs {b}: S_av={sav:.3f}  S_fused={sf:.3f}\")\n",
    "        else:\n",
    "            print(f\"[PAIR] {a} vs {b}: S_av={sav:.3f}  S_embed={se:.3f}  S_fused={sf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0edf0b",
   "metadata": {},
   "source": [
    "### Orchestration: end-to-end run\n",
    "- Set random seed for reproducibility.\n",
    "- Validate `accounts`.\n",
    "- A) Fit TF‑IDF stylometry and chunk vectors.\n",
    "- B) Compute AV similarity `S_av`.\n",
    "- C) If API key: build style embeddings `S_embed`; else skip.\n",
    "- D) Fuse scores and sweep τ for Louvain; report best communities.\n",
    "- E) Optionally merge clusters by topic and label with an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922923d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_vectorizer] n_docs=14 per_account={'acc_A': 1, 'acc_B': 1, 'acc_C': 1, 'acc_D': 1, 'acc_E': 1, 'acc_K': 1, 'acc_F': 1, 'acc_G': 1, 'acc_H': 1, 'acc_I': 1, 'acc_J': 1, 'acc_L': 1, 'acc_M': 1, 'acc_N': 1} min_df=1 max_df=0.9\n",
      "[Louvain] best_tau=0.56  modularity Q=0.596\n",
      "  community 0: ['acc_A', 'acc_D']\n",
      "  community 1: ['acc_B', 'acc_G']\n",
      "  community 2: ['acc_F']\n",
      "  community 3: ['acc_H', 'acc_I', 'acc_J', 'acc_K', 'acc_L']\n",
      "  community 4: ['acc_C', 'acc_E', 'acc_M', 'acc_N']\n",
      "[MERGE-TOPIC] merging clusters 0 and 3 (topic_sim=0.790 >= 0.70)\n",
      "[MERGE-TOPIC] merging clusters 1 and 2 (topic_sim=0.823 >= 0.70)\n",
      "[MERGE-TOPIC] merging clusters 1 and 4 (topic_sim=0.841 >= 0.70)\n",
      "[MERGE-TOPIC] merging clusters 2 and 4 (topic_sim=0.831 >= 0.70)\n",
      "\n",
      "[Topic-aware merge] Final communities after merging by topic similarity:\n",
      "  merged community 0: ['acc_A', 'acc_D', 'acc_H', 'acc_I', 'acc_J', 'acc_K', 'acc_L']\n",
      "  merged community 1: ['acc_B', 'acc_C', 'acc_E', 'acc_F', 'acc_G', 'acc_M', 'acc_N']\n",
      "\n",
      "[Cluster 0] Cintas Job Recruiters\n",
      "  signature: frequent use of 'we're hiring!'; repeated phrases like 'click to apply'; location-specific job postings; use of 'can you recommend anyone for this job?'; consistent use of 'careerarc'; short, direct sentences; use of job titles in a standardized format; emphasis on job opportunities with numbers; use of 'latest job opening' phrasing; minimal punctuation variation\n",
      "  summary: These accounts share a distinct writing style characterized by a focus on job recruitment for Cintas and related companies. They frequently use similar phrases and structures, such as 'we're hiring!' and 'click to apply', which creates a uniformity in their messaging. The accounts also consistently mention specific job locations and titles, reinforcing their purpose as job recruiters.\n",
      "\n",
      "[Cluster 1] Star Wars Bot Cluster\n",
      "  signature: frequent use of dialogue; interspersed punctuation (e.g., commas, question marks); use of ellipses and dashes; character names in dialogue; occasional use of capitalization for emphasis; descriptive phrases with complex structures; use of informal contractions; frequent references to action and movement\n",
      "  summary: These accounts exhibit a distinctive writing style characterized by a heavy reliance on dialogue and descriptive action. The use of varied punctuation, including ellipses and dashes, along with informal contractions, creates a conversational tone that is consistent across the samples.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_random_seed(42)\n",
    "\n",
    "    if not accounts:\n",
    "        raise SystemExit(\"Please populate the 'accounts' dict with your bots and tweets.\")\n",
    "\n",
    "    # A) Vectorize (stylometry)\n",
    "    vec, acc_vecs, chunks_by_acc, n_features = fit_vectorizer(accounts, ngram_range=(3, 5))\n",
    "\n",
    "    # B) AV similarity matrix\n",
    "    ids = list(accounts.keys())\n",
    "    S_av = build_similarity_matrix_AV(\n",
    "        ids, acc_vecs, n_features,\n",
    "        n_trials=1200, feat_frac=0.45, bg_per_trial=70, seed=42\n",
    "    )\n",
    "\n",
    "    S_embed = None; embedder = None\n",
    "    if API_KEY:\n",
    "        embedder = OpenAIEmbeddings(model=\"text-embedding-3-large\")   \n",
    "        acc_emb = build_account_embeddings(chunks_by_acc, embedder)\n",
    "        S_embed = cosine_matrix_from_embeddings(ids, acc_emb)\n",
    "    else:\n",
    "        print(\"[INFO] Embeddings disabled (no OPENAI_API_KEY). Using AV signal only.\")\n",
    "\n",
    "    # D) Fusion & Louvain (unknown K)\n",
    "    alpha = 0.6  # a bit more weight to AV; adjust to 0.5 if you want embeddings to pull more\n",
    "    S_fused = fuse_scores(S_av, S_embed, alpha=alpha)\n",
    "    best = sweep_tau(S_fused, ids, taus=np.linspace(0.56, 0.72, 9), resolution=1.0)\n",
    "    print(f\"[Louvain] best_tau={best['tau']:.2f}  modularity Q={best['Q']:.3f}\")\n",
    "    for cid, members in (best[\"clusters\"] or {}).items():\n",
    "        print(f\"  community {cid}: {members}\")\n",
    "\n",
    "    # E) Topic-aware merging\n",
    "    merged_clusters = best[\"clusters\"] or {}\n",
    "    if embedder is not None and merged_clusters:\n",
    "        merged_clusters = merge_clusters_by_topic(\n",
    "            merged_clusters,\n",
    "            accounts_texts=accounts,\n",
    "            embedder=embedder,\n",
    "            topic_tau=0.70,      # if two clusters' topical cosine ≥ 0.7, merge them\n",
    "            verbose=True\n",
    "        )\n",
    "        if merged_clusters != (best[\"clusters\"] or {}):\n",
    "            print(\"\\n[Topic-aware merge] Final communities after merging by topic similarity:\")\n",
    "            for cid, members in merged_clusters.items():\n",
    "                print(f\"  merged community {cid}: {members}\")\n",
    "\n",
    "    labels = {}\n",
    "    if API_KEY and merged_clusters:\n",
    "        labels = llm_label_clusters(merged_clusters, accounts, llm_model=\"gpt-4o-mini\", temperature=0.0)\n",
    "        for cid, lab in labels.items():\n",
    "            print(f\"\\n[Cluster {cid}] {lab.cluster_name}\")\n",
    "            print(\"  signature:\", \"; \".join(lab.style_signature))\n",
    "            print(\"  summary:\", lab.cohesion_summary)\n",
    "            if lab.notable_outliers:\n",
    "                print(\"  outliers:\", \", \".join(lab.notable_outliers)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
