{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94088def",
   "metadata": {},
   "source": [
    "## LLM Approach: Scientific Documentation\n",
    "\n",
    "### High-level overview\n",
    "- **Goal**: Detect clusters of related bot accounts based on writing style (stylometry), optionally fusing with embedding-based similarity and labeling clusters with an LLM.\n",
    "- **Pipeline**: data loading → normalization and chunking → TF‑IDF char n‑gram vectorization → randomized “impostors” AV similarity → optional embedding-based style/topical similarity → late fusion → graph thresholding + Leiden (default) community detection (fallback: Louvain) → optional topic-aware merges → optional LLM-based labeling.\n",
    "\n",
    "### Imports and dependencies\n",
    "- Core: `os`, `re`, `random`, `itertools`, `typing`.\n",
    "- Scientific: `numpy`, `pandas`.\n",
    "- Stylometry: `sklearn.feature_extraction.text.TfidfVectorizer` (character n‑grams), `sklearn.metrics.pairwise.cosine_similarity`.\n",
    "- Graph clustering: Leiden via `leidenalg` + `igraph` (default), fallback to NetworkX `louvain_communities` + `modularity`.\n",
    "- LLM stack (optional): `langchain_openai.OpenAIEmbeddings`, `langchain_openai.ChatOpenAI`, `langchain_core` prompt + JSON parser, `pydantic` for typed JSON.\n",
    "\n",
    "Rationale: Character n‑grams capture orthography, punctuation, casing, and function words, which are stable stylistic markers; Leiden finds well-connected, dense communities and improves Louvain’s speed and quality.\n",
    "\n",
    "### Data loading and account construction\n",
    "- Loads `OPENAI_API_KEY` via `dotenv`; if missing, embedding/LLM steps are skipped.\n",
    "- Reads `./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_sample_enriched.csv` into `df`.\n",
    "- `get_tweets_from_user(user_id)`: filters non-retweets and returns a list of `tweet_text` strings.\n",
    "- Builds `accounts: Dict[str, List[str]]` mapping synthetic IDs (`acc_A` …) to tweets from selected user IDs.\n",
    "\n",
    "Purpose: Define a small cohort of accounts with their text to run the pipeline end-to-end.\n",
    "\n",
    "### Normalization and chunking for stylometry\n",
    "- Regex normalization removes URLs and `@mentions`, lowercases, optional hashtag symbol stripping (keeps tokens), canonicalizes HTML `&amp;`, and condenses whitespace.\n",
    "- `unique_norm_texts`: normalizes and deduplicates texts.\n",
    "- `make_char_chunks(texts, target_chars≈8000, max_chunks=12)`: concatenates normalized tweets into several long chunks per account.\n",
    "  - Longer chunks stabilize character n‑gram distributions and reduce sparsity/noise.\n",
    "\n",
    "Scientific note: Character n‑grams are less topic-sensitive than word features and encode micro-level style (punctuation, casing, function words, emoji/hashtag usage).\n",
    "\n",
    "### Global TF‑IDF vectorization\n",
    "- `fit_vectorizer(acc_texts, ngram_range=(3,5), max_features=200k, sublinear_tf=True)`: builds a single global vocabulary across all chunks.\n",
    "  - Auto-adjusts `min_df`/`max_df` to be valid given the number of documents (chunks).\n",
    "  - Returns: fitted `TfidfVectorizer`, `acc_vecs` (per-account list of sparse vectors), `chunks_by_acc`, and `n_features`.\n",
    "\n",
    "Why global vocab: Ensures comparable features and global IDF statistics across accounts.\n",
    "\n",
    "### Randomized impostors AV similarity\n",
    "- Utility functions:\n",
    "  - `_rand_mask`: random feature subset mask of size `feat_frac * n_features`.\n",
    "  - `_pick_chunk`: random chunk vector for an account.\n",
    "  - `_cos_masked`: cosine similarity restricted to the masked features.\n",
    "- `impostors_score(A_chunks, B_chunks, background, n_features, n_trials, feat_frac, bg_per_trial)`:\n",
    "  - For each trial: sample features and one chunk for A and B, compute `s(A,B)`; sample `bg_per_trial` impostor accounts, compute max `s(A, impostor)`; count a win if `s(A,B)` exceeds that max.\n",
    "  - Score ∈ [0,1] = wins / n_trials.\n",
    "- `symmetric_impostors`: averages A→B and B→A.\n",
    "- `build_similarity_matrix_AV`: full pairwise symmetric matrix for all accounts.\n",
    "\n",
    "Scientific rationale: Randomized subspaces plus adversarial impostors yield a robust verification signal less biased by any fixed feature set or topical alignment.\n",
    "\n",
    "Key hyperparameters: `n_trials` (stability vs compute), `feat_frac` (feature bagging strength), `bg_per_trial` (difficulty of impostor competition).\n",
    "\n",
    "### Embedding-based style and topic representations\n",
    "- Style-focused embeddings:\n",
    "  - `style_prompt_prefix`: conditions the text to focus the embedder on stylistic aspects, not content topics.\n",
    "  - `build_account_embeddings`: embed all chunks per account (with style prompt), mean-pool, L2-normalize to unit length.\n",
    "  - `cosine_matrix_from_embeddings`: builds a [0,1]-mapped cosine matrix among accounts.\n",
    "- Topic-focused embeddings:\n",
    "  - `_cluster_topic_text`: concatenates normalized tweets of members of a cluster (no style prompt; we want topical semantics).\n",
    "  - `build_cluster_topic_embeddings`: one embedding per cluster; vectors are L2-normalized.\n",
    "- `cosine_01(a,b) = (cos(a,b) + 1)/2`: maps cosine from [-1,1] to [0,1].\n",
    "\n",
    "Separation of concerns: style embeddings aim to capture writing habits; topic embeddings capture semantic similarity for post-hoc merges.\n",
    "\n",
    "### Late fusion and community detection\n",
    "- `fuse_scores(S_av, S_embed, alpha)`: linear fusion `S_fused = α S_av + (1−α) S_embed`.\n",
    "- Community detection: threshold `S ≥ τ` to construct a weighted graph, run Leiden (default) or Louvain, compute modularity `Q`.\n",
    "- `sweep_tau`: grid over τ to maximize `Q`; returns best τ, modularity, and clusters.\n",
    "\n",
    "Scientific points: Threshold controls graph density and separability. Leiden yields well-connected, modular communities; fusion leverages complementary strengths of stylometry and embeddings.\n",
    "\n",
    "### LLM-based cluster labeling with typed JSON\n",
    "- Pydantic `ClusterLabel` schema: `cluster_name`, `style_signature` (5–10 concise markers), `cohesion_summary`, optional `notable_outliers`.\n",
    "- `LABEL_PROMPT`: instructs the LLM to focus strictly on style and return STRICT JSON.\n",
    "- `llm_label_clusters`: builds short style-focused samples for members, queries `ChatOpenAI`, parses via `JsonOutputParser` into `ClusterLabel` instances.\n",
    "\n",
    "Benefit: Structured, machine-parseable labels that articulate stylistic commonalities, not topical content.\n",
    "\n",
    "### Topic-aware cluster merging\n",
    "- `merge_clusters_by_topic`:\n",
    "  - Build topic embeddings per cluster.\n",
    "  - Compute cluster–cluster cosine similarities; merge with union-find when similarity ≥ `topic_tau`.\n",
    "  - Reindex merged groups to consecutive IDs.\n",
    "\n",
    "Purpose: Fix over-fragmentation due to graph thresholding by merging topically similar clusters that were stylistically coherent but split across τ boundaries.\n",
    "\n",
    "### Pairwise diagnostics\n",
    "- `pair_score` and `print_pair_diagnostics`: quick inspection of select account pairs to compare `S_av`, `S_embed`, and `S_fused`.\n",
    "\n",
    "Use case: Sanity checks and interpretability of the fusion behavior on hand-picked examples.\n",
    "\n",
    "### Orchestration: end-to-end run\n",
    "- Set seed for reproducibility.\n",
    "- Validate that `accounts` is populated.\n",
    "- A) Vectorize and build stylometric chunk vectors (`fit_vectorizer`).\n",
    "- B) Compute AV similarity matrix `S_av` (`build_similarity_matrix_AV`).\n",
    "- C) If API key available: build style embeddings per account → `S_embed`; else, proceed with AV only.\n",
    "- D) Fuse (`alpha ≈ 0.6`), sweep τ for Leiden/Louvain, report best `τ` and modularity `Q` with discovered communities.\n",
    "- E) If embeddings available: merge clusters by topic (`topic_tau` e.g., 0.70–0.80).\n",
    "- Optionally label merged clusters via LLM and print cluster name, style signature, and summary.\n",
    "\n",
    "### Practical notes\n",
    "- Char n‑grams: robust, topic-agnostic stylometry features for short, noisy social texts.\n",
    "- Impostors method: randomized subspaces and competition against many impostors calibrate a reliable [0,1] similarity.\n",
    "- Fusion: late fusion is simple and interpretable; tune `alpha` with validation or stability analyses.\n",
    "- Threshold τ: higher τ → tighter, smaller communities (precision↑, recall↓). Modularity sweep is a practical heuristic.\n",
    "- Randomness: trials, feature masks, chunk sampling introduce variance; seeds improve reproducibility; more trials improve stability at higher cost.\n",
    "- Data sufficiency: aim for ≥2–3 chunks per account for stable stylometry.\n",
    "- LLM labeling: ensure samples are style-focused; validate JSON parsing and outputs.\n",
    "- Complexity: AV similarity dominates runtime; consider reducing `n_trials`, `bg_per_trial`, or cohort size for large datasets.\n",
    "\n",
    "### Possible extension\n",
    "- Tune chunking (`target_chars`, `max_chunks`) for stability vs data availability.\n",
    "- Adjust `(ngram_range, feat_frac, bg_per_trial, n_trials)` for compute–stability trade-offs.\n",
    "- Experiment with different embedding models/providers and prompts for style/topic separation.\n",
    "- Try alternative clustering (Leiden, spectral) and graph construction strategies.\n",
    "- Calibrate `alpha` and `tau` using held-out linked accounts or stability criteria.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c512b5a2",
   "metadata": {},
   "source": [
    "### Imports and dependencies\n",
    "- Standard library, scientific stack, TF‑IDF for char n‑grams, cosine similarity.\n",
    "- Graph clustering via Leiden (`leidenalg` + `igraph`); fallback to NetworkX Louvain/modularity.\n",
    "- Optional LLM stack: LangChain `OpenAIEmbeddings`, `ChatOpenAI`, prompt templates, JSON parser; Pydantic for schema-validated outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "10d05f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import itertools\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities\n",
    "from networkx.algorithms.community.quality import modularity\n",
    "\n",
    "# Optional: Leiden via igraph\n",
    "try:\n",
    "    import igraph as ig\n",
    "    import leidenalg as la\n",
    "    HAS_LEIDEN = True\n",
    "except Exception:\n",
    "    HAS_LEIDEN = False\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaba2a6",
   "metadata": {},
   "source": [
    "### Data loading and account construction\n",
    "- Load environment and API key.\n",
    "- Read enriched tweets CSV.\n",
    "- Helper `get_tweets_from_user` filters original tweets.\n",
    "- Build `accounts` mapping from synthetic IDs to tweet lists for chosen user IDs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f617ba5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_32728\\871783049.py:6: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_full_enriched_english_only.csv\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['Cresci2015-FSF - 1175035327', 'Cresci2015-FSF - 1174331750', 'Cresci2015-FSF - 1127547493', 'Cresci2015-FSF - 1175854238', 'Cresci2015-INT - 349392750', 'Cresci2015-INT - 370098498', 'Cresci2015-INT - 252647855', 'Cresci2015-TWT - 91781300', 'Cresci2015-TWT - 176096243', 'Cresci2015-TWT - 17656600', 'Cresci2015-TWT - 72186699', 'Cresci2017-SocialSpambots-1 - 467182961', 'Cresci2017-SocialSpambots-1 - 2274254095', 'Cresci2017-SocialSpambots-1 - 2267349139', 'Cresci2017-SocialSpambots-1 - 1536537319', 'Cresci2017-SocialSpambots-3 - 456934703', 'Cresci2017-SocialSpambots-3 - 531163502', 'Cresci2017-SocialSpambots-3 - 335401278', 'Cresci2017-SocialSpambots-3 - 531145445', 'Cresci2017-TraditionalSpambots-1 - 69201180', 'Cresci2017-TraditionalSpambots-1 - 104896954', 'Cresci2017-TraditionalSpambots-1 - 52996113', 'Cresci2017-TraditionalSpambots-1 - 14596967', 'Cresci2017-TraditionalSpambots-3 - 3231840636', 'Cresci2017-TraditionalSpambots-3 - 3237288540', 'Cresci2017-TraditionalSpambots-3 - 2766168558', 'Cresci2017-TraditionalSpambots-3 - 3231874357', 'Cresci2017-TraditionalSpambots-4 - 72165407', 'Cresci2017-TraditionalSpambots-4 - 72208069', 'Cresci2017-TraditionalSpambots-4 - 51949191', 'Cresci2017-TraditionalSpambots-4 - 38052972', 'JournalistAttackBrianKrebs - 45779735', 'JournalistAttackBrianKrebs - 78664072', 'JournalistAttackBrianKrebs - 99319234', 'JournalistAttackBrianKrebs - 52266057', 'StarWarsBotnet - 1553296508', 'StarWarsBotnet - 1580338447', 'StarWarsBotnet - 1569742572', 'StarWarsBotnet - 1561895083'])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not API_KEY:\n",
    "    print(\"[WARN] OPENAI_API_KEY not set. Embeddings & LLM labeling will be disabled.\")\n",
    "\n",
    "df = pd.read_csv(\"./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_full_enriched_english_only.csv\")\n",
    "\n",
    "def get_tweets_from_user(user_id: int) -> List[str]:\n",
    "    return df.loc[(df[\"user_id\"] == user_id) & (df[\"is_retweet\"] == False), \"tweet_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "accounts: Dict[str, List[str]] = {}\n",
    "for folder in df['carpeta_origen'].unique():\n",
    "    accs = df[df['carpeta_origen'] == folder]['user_id'].sample(4).tolist()\n",
    "    for acc in accs:\n",
    "        acc_id = f\"{folder} - {acc}\"\n",
    "        accounts[acc_id] = get_tweets_from_user(acc)\n",
    "accounts.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5113bd02",
   "metadata": {},
   "source": [
    "### Normalization and chunking\n",
    "- Normalize tweets: lowercase, remove URLs and `@mentions`, strip `#` (keeping tokens), convert `&amp;`, and condense whitespace.\n",
    "- Deduplicate normalized texts.\n",
    "- Concatenate into character-length chunks (default ≈8k chars, up to 12) to stabilize stylometric features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "dfb88b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_RE = re.compile(r'@\\w+')\n",
    "WS_RE = re.compile(r'\\s+')\n",
    "\n",
    "def normalize_tweet(t: str, keep_hashtags: bool = True) -> str:\n",
    "    t = t.lower()\n",
    "    t = URL_RE.sub(' ', t)\n",
    "    t = MENTION_RE.sub(' ', t)\n",
    "    if keep_hashtags:\n",
    "        t = t.replace('#', '')\n",
    "    t = t.replace('&amp;', '&')\n",
    "    t = WS_RE.sub(' ', t).strip()\n",
    "    return t\n",
    "\n",
    "def unique_norm_texts(texts: List[str]) -> List[str]:\n",
    "    seen, out = set(), []\n",
    "    for s in texts:\n",
    "        n = normalize_tweet(s)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); out.append(n)\n",
    "    return out\n",
    "\n",
    "def make_char_chunks(texts: List[str], target_chars: int = 8000, max_chunks: int = 12) -> List[str]:\n",
    "    \"\"\"\n",
    "    Larger target_chars -> more stable AV (aim for >= 2–3 chunks per account).\n",
    "    \"\"\"\n",
    "    msgs = unique_norm_texts(texts)\n",
    "    if not msgs:\n",
    "        return []\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for m in msgs:\n",
    "        cur.append(m); cur_len += len(m) + 1\n",
    "        if cur_len >= target_chars:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_len = [], 0\n",
    "            if len(chunks) >= max_chunks:\n",
    "                break\n",
    "    if cur and len(chunks) < max_chunks:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks if chunks else [\" \".join(msgs)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "803beeac",
   "metadata": {},
   "source": [
    "### Global TF‑IDF stylometry\n",
    "- Build a single character n‑gram TF‑IDF vocabulary across all chunks (`(3,5)`-grams, `sublinear_tf=True`).\n",
    "- Auto-adjust `min_df`/`max_df` to be valid for the observed number of documents (chunks).\n",
    "- Return the vectorizer, per-account chunk vectors, chunk texts, and feature count.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0f771dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(acc_texts: Dict[str, List[str]], ngram_range: Tuple[int, int] = (3, 5), max_features: int = 200_000, min_df=None, max_df=None, sublinear_tf: bool = True):\n",
    "    \"\"\"\n",
    "    Build one global TF-IDF vocab across all account chunks (robust for AV).\n",
    "    Auto-adjust min_df/max_df to avoid conflicts (max_df_docs >= min_df_docs).\n",
    "    \"\"\"\n",
    "    docs, ids, chunks_by_acc = [], [], {}\n",
    "    per_acc = {}\n",
    "    for aid, texts in acc_texts.items():\n",
    "        chs = make_char_chunks(texts, target_chars=8000, max_chunks=12)\n",
    "        chunks_by_acc[aid] = chs[:]\n",
    "        per_acc[aid] = len(chs)\n",
    "        for ch in chs:\n",
    "            docs.append(ch); ids.append(aid)\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < 2:\n",
    "        raise ValueError(\"Too few chunks. Add data or reduce target_chars.\")\n",
    "\n",
    "    umin, umax = min_df, max_df\n",
    "    if min_df is None: min_df = max(1, int(0.01 * n_docs))\n",
    "    if max_df is None: max_df = 0.90\n",
    "    def _to_docs(val, n): return int(val * n) if isinstance(val, float) else int(val)\n",
    "    max_docs = _to_docs(max_df, n_docs)\n",
    "    min_docs = _to_docs(min_df, n_docs) if isinstance(min_df, int) else _to_docs(min_df, n_docs)\n",
    "    if max_docs < min_docs:\n",
    "        min_docs = max(1, max_docs)\n",
    "        min_df = min_docs if not isinstance(umin, float) else max(umin, min_docs / n_docs)\n",
    "    max_docs = _to_docs(max_df, n_docs)\n",
    "    min_docs = _to_docs(min_df, n_docs) if isinstance(min_df, int) else _to_docs(min_df, n_docs)\n",
    "    if max_docs < min_docs:\n",
    "        max_df = min(0.99, (min_docs + 1) / n_docs) if isinstance(max_df, float) else (min_docs + 1)\n",
    "\n",
    "    print(f\"[fit_vectorizer] n_docs={n_docs} per_account={per_acc} min_df={min_df} max_df={max_df}\")\n",
    "\n",
    "    vec = TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=ngram_range,\n",
    "        min_df=min_df, max_df=max_df, max_features=max_features,\n",
    "        norm=\"l2\", sublinear_tf=sublinear_tf\n",
    "    )\n",
    "    \n",
    "    X = vec.fit_transform(docs)\n",
    "\n",
    "    acc_vecs = {}\n",
    "    for i, aid in enumerate(ids):\n",
    "        acc_vecs.setdefault(aid, []).append(X[i])\n",
    "    n_features = len(vec.get_feature_names_out())\n",
    "    return vec, acc_vecs, chunks_by_acc, n_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0efff0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Candidate prefilter for scalability (top-K neighbors via cosine on aggregated TF-IDF)\n",
    "from scipy.sparse import vstack as sp_vstack, issparse, csr_matrix\n",
    "\n",
    "def aggregate_account_vectors(ids: List[str], acc_vecs: Dict[str, List]) -> Tuple[np.ndarray, List[str]]:\n",
    "    \"\"\"\n",
    "    Build a single L2-normalized mean vector per account from its chunk vectors (sparse CSR).\n",
    "    \"\"\"\n",
    "    agg = []\n",
    "    for aid in ids:\n",
    "        vecs = acc_vecs[aid]\n",
    "        if not vecs:\n",
    "            raise ValueError(f\"Account {aid} has no chunk vectors\")\n",
    "        if len(vecs) == 1:\n",
    "            m = vecs[0]\n",
    "        else:\n",
    "            M = sp_vstack(vecs)\n",
    "            m = M.mean(axis=0)  # returns numpy.matrix\n",
    "        # ensure CSR sparse row\n",
    "        if not issparse(m):\n",
    "            m = csr_matrix(m)\n",
    "        else:\n",
    "            m = m.tocsr()\n",
    "        # sklearn cosine expects L2 norm; normalize here\n",
    "        denom = float(np.sqrt(m.multiply(m).sum()) + 1e-12)\n",
    "        m = m / denom\n",
    "        agg.append(m)\n",
    "    return sp_vstack(agg), ids\n",
    "\n",
    "def build_candidate_pairs(ids: List[str], acc_vecs: Dict[str, List], max_candidates: int = 50, min_cos: float = 0.05) -> List[Tuple[int, int]]:\n",
    "    \"\"\"\n",
    "    Use a cheap cosine on per-account mean TF-IDF vectors to produce a sparse candidate set\n",
    "    of likely similar pairs. Returns list of (i,j) with i < j.\n",
    "    \"\"\"\n",
    "    X, _ = aggregate_account_vectors(ids, acc_vecs)\n",
    "    nn = NearestNeighbors(n_neighbors=min(max_candidates + 1, len(ids)), metric=\"cosine\", algorithm=\"brute\")\n",
    "    nn.fit(X)\n",
    "    dists, indices = nn.kneighbors(X, return_distance=True)\n",
    "    pairs = set()\n",
    "    for i in range(len(ids)):\n",
    "        for d, j in zip(dists[i], indices[i]):\n",
    "            if i == j:\n",
    "                continue\n",
    "            cos = 1.0 - float(d)\n",
    "            if cos >= min_cos:\n",
    "                a, b = (i, j) if i < j else (j, i)\n",
    "                pairs.add((a, b))\n",
    "    return sorted(pairs)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261db26e",
   "metadata": {},
   "source": [
    "### Impostors AV similarity and randomized subspaces\n",
    "- Randomly sample feature subsets (`feat_frac`) and one chunk per account per trial.\n",
    "- Compute masked cosine `s(A,B)` vs the maximum `s(A, impostor)` over sampled background accounts.\n",
    "- Score is the fraction of trials where A beats impostors when paired with B; symmetrize A↔B.\n",
    "- Build the full similarity matrix `S_av` across all accounts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "481c2e6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def _precompute_masks(n_features: int, keep_frac: float, n_masks: int, rng: np.random.Generator) -> List[np.ndarray]:\n",
    "    k = max(1, int(n_features * keep_frac))\n",
    "    masks = [rng.choice(n_features, size=k, replace=False) for _ in range(n_masks)]\n",
    "    return masks\n",
    "\n",
    "def _cos_masked_cols(a, b, cols: np.ndarray) -> float:\n",
    "    return float(cosine_similarity(a[:, cols], b[:, cols])[0, 0])\n",
    "\n",
    "def _pick_chunk(vecs: List, rng: np.random.Generator) -> np.ndarray:\n",
    "    if not vecs: raise ValueError(\"Account has no chunk vectors.\")\n",
    "    return vecs[int(rng.integers(len(vecs)))]\n",
    "\n",
    "def impostors_score(A_chunks: List, B_chunks: List, background: List[List],\n",
    "                    n_features: int, n_trials=1200, feat_frac=0.45, bg_per_trial=70,\n",
    "                    masks_cols: Optional[List[np.ndarray]] = None,\n",
    "                    early_stop: bool = True, eps: float = 0.02, delta: float = 1e-3,\n",
    "                    rng: Optional[np.random.Generator] = None) -> float:\n",
    "    if not background: return 0.0\n",
    "    rng = rng or np.random.default_rng()\n",
    "    wins, poolN = 0, len(background)\n",
    "    k = min(bg_per_trial, poolN)\n",
    "    if masks_cols is None:\n",
    "        masks_cols = _precompute_masks(n_features, feat_frac, n_trials, rng)\n",
    "    bound_const = np.sqrt(0.5 * np.log(2.0 / max(delta, 1e-12))) if early_stop else None\n",
    "    for t in range(1, n_trials + 1):\n",
    "        cols = masks_cols[(t - 1) % len(masks_cols)]\n",
    "        vA, vB = _pick_chunk(A_chunks, rng), _pick_chunk(B_chunks, rng)\n",
    "        sAB = _cos_masked_cols(vA, vB, cols)\n",
    "        idx = rng.choice(poolN, size=k, replace=False)\n",
    "        sAI = [_cos_masked_cols(vA, _pick_chunk(background[j], rng), cols) for j in idx]\n",
    "        if sAB > max(sAI, default=-1.0):\n",
    "            wins += 1\n",
    "        if early_stop and t >= 30:\n",
    "            width = bound_const / np.sqrt(t)\n",
    "            # stop when additional trials unlikely to change decision by > eps\n",
    "            if width <= eps:\n",
    "                break\n",
    "    return wins / max(1, t)\n",
    "\n",
    "def symmetric_impostors(A_chunks, B_chunks, background, n_features, n_trials=1200, feat_frac=0.45, bg_per_trial=70,\n",
    "                        masks_cols: Optional[List[np.ndarray]] = None, early_stop: bool = True, eps: float = 0.02, delta: float = 1e-3,\n",
    "                        rng: Optional[np.random.Generator] = None) -> float:\n",
    "    rng = rng or np.random.default_rng()\n",
    "    s1 = impostors_score(A_chunks, B_chunks, background, n_features, n_trials, feat_frac, bg_per_trial,\n",
    "                         masks_cols=masks_cols, early_stop=early_stop, eps=eps, delta=delta, rng=rng)\n",
    "    s2 = impostors_score(B_chunks, A_chunks, background, n_features, n_trials, feat_frac, bg_per_trial,\n",
    "                         masks_cols=masks_cols, early_stop=early_stop, eps=eps, delta=delta, rng=rng)\n",
    "    return 0.5 * (s1 + s2)\n",
    "\n",
    "def build_similarity_matrix_AV(ids: List[str], acc_vecs: Dict[str, List], n_features: int, n_trials=1200, feat_frac=0.45, bg_per_trial=70,\n",
    "                               seed=42, candidate_pairs: Optional[List[Tuple[int, int]]] = None, n_jobs: int = 1,\n",
    "                               early_stop: bool = True, eps: float = 0.02, delta: float = 1e-3, n_masks: int = 256) -> np.ndarray:\n",
    "    rng = np.random.default_rng(seed)\n",
    "    n = len(ids)\n",
    "    S = np.eye(n, dtype=float)\n",
    "    chunks = {i: acc_vecs[i] for i in ids}\n",
    "    masks_cols = _precompute_masks(n_features, feat_frac, n_masks, rng)\n",
    "\n",
    "    pairs = candidate_pairs if candidate_pairs is not None else list(itertools.combinations(range(n), 2))\n",
    "\n",
    "    def _compute(i, j):\n",
    "        Ai, Bj = ids[i], ids[j]\n",
    "        back = [chunks[k] for k in ids if k not in (Ai, Bj)]\n",
    "        local_rng = np.random.default_rng(int(seed) + i * 10007 + j)\n",
    "        s = symmetric_impostors(\n",
    "            chunks[Ai], chunks[Bj], back, n_features,\n",
    "            n_trials=n_trials, feat_frac=feat_frac, bg_per_trial=bg_per_trial,\n",
    "            masks_cols=masks_cols, early_stop=early_stop, eps=eps, delta=delta, rng=local_rng\n",
    "        )\n",
    "        return i, j, s\n",
    "\n",
    "    if n_jobs and n_jobs != 1:\n",
    "        results = Parallel(n_jobs=n_jobs, prefer=\"processes\")(\n",
    "            delayed(_compute)(i, j) for (i, j) in pairs\n",
    "        )\n",
    "        for i, j, s in results:\n",
    "            S[i, j] = S[j, i] = s\n",
    "    else:\n",
    "        for i, j in pairs:\n",
    "            _, _, s = _compute(i, j)\n",
    "            S[i, j] = S[j, i] = s\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aac49307",
   "metadata": {},
   "source": [
    "### Embeddings for style and topic\n",
    "- Style embeddings: prefix text with a style-only instruction, embed chunks, mean-pool, L2-normalize; derive `S_embed`.\n",
    "- Topic embeddings: concatenate normalized texts per cluster (no style prompt), embed and L2-normalize; used to compare and merge clusters by topic.\n",
    "- Cosine values are mapped to [0,1] via `(cos + 1)/2`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d53fc2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def style_prompt_prefix(text: str) -> str:\n",
    "    return (\n",
    "        \"Instruction: Create an embedding that captures WRITING STYLE only \"\n",
    "        \"(punctuation, casing, rhythm, emoji/hashtag usage, function words), not topical content.\\n\\n\"\n",
    "        \"TEXT:\\n\" + text\n",
    "    )\n",
    "\n",
    "def build_account_embeddings(chunks_by_acc: Dict[str, List[str]], embedder) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"Per-account L2-normalized mean of chunk embeddings (STYLE-focused).\"\"\"\n",
    "    acc_vec = {}\n",
    "    for aid, chunks in chunks_by_acc.items():\n",
    "        if not chunks:\n",
    "            acc_vec[aid] = None\n",
    "            continue\n",
    "        docs = [style_prompt_prefix(ch) for ch in chunks]\n",
    "        embs = embedder.embed_documents(docs)      # LangChain OpenAIEmbeddings # :contentReference[oaicite:10]{index=10}\n",
    "        M = np.array(embs, dtype=np.float32)\n",
    "        mu = M.mean(axis=0); mu /= (np.linalg.norm(mu) + 1e-12)\n",
    "        acc_vec[aid] = mu\n",
    "    return acc_vec\n",
    "\n",
    "def cosine_matrix_from_embeddings(ids: List[str], acc_emb: Dict[str, np.ndarray]) -> np.ndarray:\n",
    "    mat = np.vstack([acc_emb[i] for i in ids])\n",
    "    C = np.clip(mat @ mat.T, -1.0, 1.0)\n",
    "    return 0.5 * (C + 1.0)\n",
    "\n",
    "def _cluster_topic_text(cluster_members: List[str], accounts_texts: Dict[str, List[str]], max_chars_per_acc=1200) -> str:\n",
    "    \"\"\"\n",
    "    Build a short, topic-focused sample for a whole cluster by concatenating\n",
    "    (normalized) tweets of its members. We intentionally DO NOT use the style\n",
    "    prompt here; we want topical semantics.\n",
    "    \"\"\"\n",
    "    out, total = [], 0\n",
    "    for aid in cluster_members:\n",
    "        for t in accounts_texts[aid]:\n",
    "            t = normalize_tweet(t) \n",
    "            if total + len(t) + 1 > max_chars_per_acc * len(cluster_members):\n",
    "                break\n",
    "            out.append(t); total += len(t) + 1\n",
    "    return \"\\n\".join(out) if out else \"\"\n",
    "\n",
    "def build_cluster_topic_embeddings(clusters: Dict[int, List[str]], accounts_texts: Dict[str, List[str]], embedder) -> Dict[int, np.ndarray]:\n",
    "    \"\"\"\n",
    "    One embedding per cluster using raw (normalized) text concatenated.\n",
    "    This captures TOPIC similarity (not style).\n",
    "    \"\"\"\n",
    "    cl_vec = {}\n",
    "    docs = []\n",
    "    order = []\n",
    "    for cid, members in clusters.items():\n",
    "        txt = _cluster_topic_text(members, accounts_texts)\n",
    "        if not txt: continue\n",
    "        docs.append(txt); order.append(cid)\n",
    "    if not docs:\n",
    "        return {}\n",
    "    embs = embedder.embed_documents(docs)\n",
    "    for i, cid in enumerate(order):\n",
    "        v = np.array(embs[i], dtype=np.float32)\n",
    "        v /= (np.linalg.norm(v) + 1e-12)\n",
    "        cl_vec[cid] = v\n",
    "    return cl_vec\n",
    "\n",
    "def cosine_01(a: np.ndarray, b: np.ndarray) -> float:\n",
    "    c = float(np.clip(np.dot(a, b), -1.0, 1.0))\n",
    "    return 0.5 * (c + 1.0)  # map [-1,1] -> [0,1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f493de37",
   "metadata": {},
   "source": [
    "### Scaling notes (performance knobs)\n",
    "- Top-K candidate prefilter: build cheap cosine neighbors on per-account mean TF‑IDF; only run AV on these pairs.\n",
    "  - Knobs: `max_candidates` (default 50), `min_cos` (default 0.05).\n",
    "- Early stopping in AV trials with Hoeffding bound.\n",
    "  - Knobs: `early_stop=True`, `eps=0.02`, `delta=1e-3`, `n_trials` cap.\n",
    "- Parallelism for pair scoring via joblib processes.\n",
    "  - Knobs: `n_jobs` (default ≈ half cores), adjust per machine.\n",
    "- Random subspace reuse: precompute column subsets to reduce per‑trial overhead.\n",
    "  - Knobs: `n_masks` (default 256), `feat_frac`.\n",
    "- Practical tip: increase `max_candidates` or lower `min_cos` to recover recall if you scale to many accounts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07691ab3",
   "metadata": {},
   "source": [
    "### Fusion and community detection (Leiden/Louvain)\n",
    "- Fuse stylometric AV and embedding similarities: `S_fused = α S_av + (1−α) S_embed`.\n",
    "- Build a thresholded weighted graph at `S ≥ τ` and run Leiden (default) or Louvain; compute modularity `Q`.\n",
    "- Sweep τ to find the partition with maximal modularity.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "49d131cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuse_scores(S_av: np.ndarray, S_embed: Optional[np.ndarray] = None, alpha: float = 0.7) -> np.ndarray:\n",
    "    if S_embed is None:\n",
    "        return S_av\n",
    "    return alpha * S_av + (1.0 - alpha) * S_embed\n",
    "\n",
    "def louvain_partition(S: np.ndarray, ids: List[str], tau: float = 0.65, resolution: float = 1.0):\n",
    "    \"\"\"\n",
    "    Threshold S at tau -> weighted graph -> Louvain communities -> modularity Q.\n",
    "    NetworkX louvain_communities & modularity are used.\n",
    "    \"\"\"\n",
    "    G = nx.Graph(); G.add_nodes_from(ids)\n",
    "    n = len(ids)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            w = float(S[i, j])\n",
    "            if w >= tau:\n",
    "                G.add_edge(ids[i], ids[j], weight=w)\n",
    "    if G.number_of_edges() == 0:\n",
    "        return {}, 0.0\n",
    "    comms = louvain_communities(G, weight=\"weight\", resolution=resolution, seed=42)\n",
    "    Q = modularity(G, comms, weight=\"weight\")\n",
    "    clusters = {i: sorted(list(c)) for i, c in enumerate(comms)}\n",
    "    return clusters, Q\n",
    "\n",
    "def leiden_partition(S: np.ndarray, ids: List[str], tau: float = 0.65, resolution: float = 1.0):\n",
    "    \"\"\"\n",
    "    Threshold S at tau -> weighted graph -> Leiden communities -> modularity Q.\n",
    "    Implemented via igraph + leidenalg (RBConfigurationVertexPartition) with weights.\n",
    "    \"\"\"\n",
    "    G = nx.Graph(); G.add_nodes_from(ids)\n",
    "    n = len(ids)\n",
    "    for i in range(n):\n",
    "        for j in range(i+1, n):\n",
    "            w = float(S[i, j])\n",
    "            if w >= tau:\n",
    "                G.add_edge(ids[i], ids[j], weight=w)\n",
    "    if G.number_of_edges() == 0:\n",
    "        return {}, 0.0\n",
    "\n",
    "    # Convert to igraph\n",
    "    node_to_idx = {node: i for i, node in enumerate(ids)}\n",
    "    edges = []\n",
    "    weights = []\n",
    "    for u, v, d in G.edges(data=True):\n",
    "        edges.append((node_to_idx[u], node_to_idx[v]))\n",
    "        weights.append(float(d.get(\"weight\", 1.0)))\n",
    "    g = ig.Graph(n=len(ids), edges=edges, directed=False)\n",
    "    g.es[\"weight\"] = weights\n",
    "\n",
    "    part = la.find_partition(\n",
    "        g,\n",
    "        la.RBConfigurationVertexPartition,\n",
    "        weights=\"weight\",\n",
    "        resolution_parameter=resolution,\n",
    "        n_iterations=-1,\n",
    "        seed=42,\n",
    "    )\n",
    "\n",
    "    clusters = {}\n",
    "    for node, comm_id in zip(ids, part.membership):\n",
    "        clusters.setdefault(comm_id, []).append(node)\n",
    "    clusters = {i: sorted(members) for i, members in enumerate(clusters.values())}\n",
    "\n",
    "    Q = modularity(G, [set(m) for m in clusters.values()], weight=\"weight\")\n",
    "    return clusters, Q\n",
    "\n",
    "def sweep_tau(S: np.ndarray, ids: List[str], taus=np.linspace(0.56, 0.72, 9), resolution=1.0, method: str = \"leiden\"):\n",
    "    if method == \"leiden\":\n",
    "        if not globals().get(\"HAS_LEIDEN\", False):\n",
    "            print(\"[WARN] Leiden not available, falling back to Louvain.\")\n",
    "            part_fn = louvain_partition\n",
    "        else:\n",
    "            part_fn = leiden_partition\n",
    "    else:\n",
    "        part_fn = louvain_partition\n",
    "\n",
    "    best = {\"tau\": None, \"Q\": -1, \"clusters\": None}\n",
    "    for tau in taus:\n",
    "        cs, Q = part_fn(S, ids, tau=tau, resolution=resolution)\n",
    "        if Q is not None and Q > best[\"Q\"]:\n",
    "            best = {\"tau\": tau, \"Q\": Q, \"clusters\": cs}\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1214e5b2",
   "metadata": {},
   "source": [
    "### LLM-based cluster labeling (structured JSON)\n",
    "- Pydantic schema `ClusterLabel` defines the target JSON structure.\n",
    "- Prompt instructs the LLM to focus on stylistic markers only.\n",
    "- For each cluster: build short style-focused samples, call the LLM, and parse with `JsonOutputParser`.\n",
    "\n",
    "Outcome: machine-parseable labels with a concise name, style signature, and cohesion summary.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f7e05178",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClusterLabel(BaseModel):\n",
    "    cluster_name: str = Field(..., description=\"Short English name for the botnet cluster\")\n",
    "    style_signature: List[str] = Field(..., description=\"5–10 concise style markers shared by the cluster\")\n",
    "    cohesion_summary: str = Field(..., description=\"2–3 sentences explaining WHY these accounts belong together stylistically (NOT topics)\")\n",
    "    notable_outliers: List[str] = Field(default_factory=list, description=\"Optional: account IDs that partially fit or diverge\")\n",
    "\n",
    "def _short_style_sample(texts: List[str], max_chars: int = 1500) -> str:\n",
    "    buf, total = [], 0\n",
    "    for t in texts:\n",
    "        if not t: continue\n",
    "        if total + len(t) + 1 > max_chars: break\n",
    "        buf.append(t); total += len(t) + 1\n",
    "    return \"\\n\".join(buf) if buf else \"\"\n",
    "\n",
    "LABEL_PROMPT = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a forensic writing analyst. Label a cluster of bot accounts discovered via STYLOMETRIC similarity.\n",
    "Focus ONLY on WRITING STYLE (punctuation, casing, emoji/hashtag usage, function words, character n-grams, formatting quirks), NOT topics.\n",
    "\n",
    "Return STRICT JSON with this schema:\n",
    "{schema}\n",
    "\n",
    "For each member you receive:\n",
    "- account_id\n",
    "- a short style-focused sample (possibly truncated)\n",
    "\n",
    "Members:\n",
    "{members}\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "def llm_label_clusters(clusters: Dict[int, List[str]],\n",
    "                       accounts_texts: Dict[str, List[str]],\n",
    "                       llm_model: str = \"gpt-4o-mini\",\n",
    "                       temperature: float = 0.0) -> Dict[int, ClusterLabel]:\n",
    "    if not API_KEY:\n",
    "        print(\"[INFO] Skipping LLM labeling (no OPENAI_API_KEY).\")\n",
    "        return {}\n",
    "    llm = ChatOpenAI(model=llm_model, temperature=temperature)\n",
    "    parser = JsonOutputParser(pydantic_object=ClusterLabel)\n",
    "    results: Dict[int, ClusterLabel] = {}\n",
    "    for cid, members in clusters.items():\n",
    "        lines = []\n",
    "        for aid in members:\n",
    "            sample = _short_style_sample([normalize_tweet(t) for t in accounts_texts[aid]], max_chars=1400)\n",
    "            lines.append(f\"- {aid}:\\n\\\"\\\"\\\"\\n{sample}\\n\\\"\\\"\\\"\")\n",
    "        prompt = LABEL_PROMPT.format_messages(\n",
    "            schema=parser.get_format_instructions(),\n",
    "            members=\"\\n\".join(lines)\n",
    "        )\n",
    "        resp = llm.invoke(prompt)\n",
    "        out = parser.parse(resp.content)\n",
    "        if isinstance(out, dict):  \n",
    "            out = ClusterLabel(**out)\n",
    "        results[cid] = out\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9910d9c1",
   "metadata": {},
   "source": [
    "### Topic-aware cluster merging\n",
    "- Build topic embeddings per cluster by concatenating normalized member texts (no style prompt).\n",
    "- Merge clusters via union-find when cluster–cluster cosine ≥ `topic_tau`.\n",
    "- Reindex merged groups to consecutive IDs.\n",
    "\n",
    "Purpose: correct over-fragmentation from the thresholded graph by merging topically coherent clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4ad09395",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_clusters_by_topic(clusters: Dict[int, List[str]], accounts_texts: Dict[str, List[str]], embedder, topic_tau: float = 0.80, verbose: bool = True) -> Dict[int, List[str]]:\n",
    "    \"\"\"\n",
    "    Merge clusters if their TOPIC embeddings cosine ≥ topic_tau.\n",
    "    - Build one \"topic\" embedding per cluster (concatenate normalized texts).\n",
    "    - Compute cosine similarity among clusters.\n",
    "    - Union-Find merge for all pairs above threshold (transitive closure).\n",
    "    \"\"\"\n",
    "    if not clusters:\n",
    "        return clusters\n",
    "    # Build cluster-level topic embeddings\n",
    "    cl_emb = build_cluster_topic_embeddings(clusters, accounts_texts, embedder)\n",
    "    if len(cl_emb) <= 1:\n",
    "        return clusters\n",
    "\n",
    "    # Union-Find structure\n",
    "    parent = {cid: cid for cid in clusters}\n",
    "    def find(x):\n",
    "        while parent[x] != x:\n",
    "            parent[x] = parent[parent[x]]\n",
    "            x = parent[x]\n",
    "        return x\n",
    "    def union(a, b):\n",
    "        ra, rb = find(a), find(b)\n",
    "        if ra != rb:\n",
    "            parent[rb] = ra\n",
    "\n",
    "    # Compare all pairs\n",
    "    cids = sorted(cl_emb.keys())\n",
    "    for i in range(len(cids)):\n",
    "        for j in range(i+1, len(cids)):\n",
    "            ci, cj = cids[i], cids[j]\n",
    "            s = cosine_01(cl_emb[ci], cl_emb[cj])   \n",
    "            if s >= topic_tau:\n",
    "                if verbose:\n",
    "                    print(f\"[MERGE-TOPIC] merging clusters {ci} and {cj} (topic_sim={s:.3f} >= {topic_tau:.2f})\")\n",
    "                union(ci, cj)\n",
    "\n",
    "    # Build merged mapping\n",
    "    groups = {}\n",
    "    for cid in clusters:\n",
    "        root = find(cid)\n",
    "        groups.setdefault(root, [])\n",
    "        groups[root].extend(clusters[cid])\n",
    "\n",
    "    # Reindex cluster ids 0..K-1\n",
    "    merged = {}\n",
    "    for new_id, (_, members) in enumerate(sorted(groups.items(), key=lambda kv: tuple(kv[1]))):\n",
    "        merged[new_id] = sorted(set(members))\n",
    "    return merged"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1416e2",
   "metadata": {},
   "source": [
    "### Pairwise diagnostics\n",
    "Compare specific account pairs across signals:\n",
    "- `S_av`: stylometric impostors similarity\n",
    "- `S_embed`: style-embedding cosine (if available)\n",
    "- `S_fused`: late-fused score\n",
    "\n",
    "Use for sanity checks and qualitative inspection of fusion behavior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8cc10a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pair_score(ids: List[str], S: np.ndarray, a: str, b: str) -> float:\n",
    "    idx = {aid: i for i, aid in enumerate(ids)}\n",
    "    return float(S[idx[a], idx[b]])\n",
    "\n",
    "def print_pair_diagnostics(ids: List[str], S_av: np.ndarray, S_embed: Optional[np.ndarray], S_fused: np.ndarray,\n",
    "                           pairs: List[Tuple[str, str]]):\n",
    "    for a, b in pairs:\n",
    "        sav = pair_score(ids, S_av, a, b)\n",
    "        se = pair_score(ids, S_embed, a, b) if S_embed is not None else None\n",
    "        sf = pair_score(ids, S_fused, a, b)\n",
    "        if se is None:\n",
    "            print(f\"[PAIR] {a} vs {b}: S_av={sav:.3f}  S_fused={sf:.3f}\")\n",
    "        else:\n",
    "            print(f\"[PAIR] {a} vs {b}: S_av={sav:.3f}  S_embed={se:.3f}  S_fused={sf:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d0edf0b",
   "metadata": {},
   "source": [
    "### Orchestration: end-to-end run\n",
    "- Set random seed for reproducibility.\n",
    "- Validate `accounts`.\n",
    "- A) Fit TF‑IDF stylometry and chunk vectors.\n",
    "- B) Compute AV similarity `S_av`.\n",
    "- C) If API key: build style embeddings `S_embed`; else skip.\n",
    "- D) Fuse scores and sweep τ for Louvain; report best communities.\n",
    "- E) Optionally merge clusters by topic and label with an LLM.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "922923d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_vectorizer] n_docs=219 per_account={'Cresci2015-FSF - 1175035327': 1, 'Cresci2015-FSF - 1174331750': 1, 'Cresci2015-FSF - 1127547493': 1, 'Cresci2015-FSF - 1175854238': 1, 'Cresci2015-INT - 349392750': 1, 'Cresci2015-INT - 370098498': 1, 'Cresci2015-INT - 252647855': 1, 'Cresci2015-TWT - 91781300': 1, 'Cresci2015-TWT - 176096243': 5, 'Cresci2015-TWT - 17656600': 4, 'Cresci2015-TWT - 72186699': 12, 'Cresci2017-SocialSpambots-1 - 467182961': 1, 'Cresci2017-SocialSpambots-1 - 2274254095': 1, 'Cresci2017-SocialSpambots-1 - 2267349139': 1, 'Cresci2017-SocialSpambots-1 - 1536537319': 12, 'Cresci2017-SocialSpambots-3 - 456934703': 12, 'Cresci2017-SocialSpambots-3 - 531163502': 12, 'Cresci2017-SocialSpambots-3 - 335401278': 12, 'Cresci2017-SocialSpambots-3 - 531145445': 12, 'Cresci2017-TraditionalSpambots-1 - 69201180': 9, 'Cresci2017-TraditionalSpambots-1 - 104896954': 2, 'Cresci2017-TraditionalSpambots-1 - 52996113': 12, 'Cresci2017-TraditionalSpambots-1 - 14596967': 12, 'Cresci2017-TraditionalSpambots-3 - 3231840636': 12, 'Cresci2017-TraditionalSpambots-3 - 3237288540': 12, 'Cresci2017-TraditionalSpambots-3 - 2766168558': 12, 'Cresci2017-TraditionalSpambots-3 - 3231874357': 12, 'Cresci2017-TraditionalSpambots-4 - 72165407': 1, 'Cresci2017-TraditionalSpambots-4 - 72208069': 3, 'Cresci2017-TraditionalSpambots-4 - 51949191': 2, 'Cresci2017-TraditionalSpambots-4 - 38052972': 2, 'JournalistAttackBrianKrebs - 45779735': 4, 'JournalistAttackBrianKrebs - 78664072': 1, 'JournalistAttackBrianKrebs - 99319234': 12, 'JournalistAttackBrianKrebs - 52266057': 9, 'StarWarsBotnet - 1553296508': 1, 'StarWarsBotnet - 1580338447': 7, 'StarWarsBotnet - 1569742572': 1, 'StarWarsBotnet - 1561895083': 1} min_df=2 max_df=0.9\n",
      "[AV] candidate pairs: 600 (out of 741)\n",
      "[Leiden] best_tau=0.58  modularity Q=0.854\n",
      "  community 0: ['Cresci2015-FSF - 1175035327', 'Cresci2015-INT - 252647855', 'Cresci2015-INT - 349392750', 'Cresci2015-INT - 370098498']\n",
      "  community 1: ['Cresci2015-FSF - 1174331750', 'Cresci2015-TWT - 176096243', 'Cresci2015-TWT - 72186699']\n",
      "  community 2: ['Cresci2015-FSF - 1127547493']\n",
      "  community 3: ['Cresci2015-FSF - 1175854238']\n",
      "  community 4: ['Cresci2015-TWT - 91781300', 'Cresci2017-TraditionalSpambots-1 - 104896954']\n",
      "  community 5: ['Cresci2015-TWT - 17656600', 'JournalistAttackBrianKrebs - 45779735', 'JournalistAttackBrianKrebs - 99319234', 'StarWarsBotnet - 1580338447']\n",
      "  community 6: ['Cresci2017-SocialSpambots-1 - 2267349139', 'Cresci2017-SocialSpambots-1 - 2274254095', 'Cresci2017-SocialSpambots-1 - 467182961']\n",
      "  community 7: ['Cresci2017-SocialSpambots-1 - 1536537319']\n",
      "  community 8: ['Cresci2017-SocialSpambots-3 - 335401278', 'Cresci2017-SocialSpambots-3 - 456934703', 'Cresci2017-SocialSpambots-3 - 531145445', 'Cresci2017-SocialSpambots-3 - 531163502']\n",
      "  community 9: ['Cresci2017-TraditionalSpambots-1 - 69201180']\n",
      "  community 10: ['Cresci2017-TraditionalSpambots-1 - 52996113']\n",
      "  community 11: ['Cresci2017-TraditionalSpambots-1 - 14596967']\n",
      "  community 12: ['Cresci2017-TraditionalSpambots-3 - 3231840636']\n",
      "  community 13: ['Cresci2017-TraditionalSpambots-3 - 3237288540']\n",
      "  community 14: ['Cresci2017-TraditionalSpambots-3 - 2766168558', 'Cresci2017-TraditionalSpambots-3 - 3231874357']\n",
      "  community 15: ['Cresci2017-TraditionalSpambots-4 - 51949191', 'Cresci2017-TraditionalSpambots-4 - 72165407', 'Cresci2017-TraditionalSpambots-4 - 72208069']\n",
      "  community 16: ['Cresci2017-TraditionalSpambots-4 - 38052972']\n",
      "  community 17: ['JournalistAttackBrianKrebs - 78664072']\n",
      "  community 18: ['JournalistAttackBrianKrebs - 52266057']\n",
      "  community 19: ['StarWarsBotnet - 1553296508', 'StarWarsBotnet - 1569742572']\n",
      "  community 20: ['StarWarsBotnet - 1561895083']\n",
      "[MERGE-TOPIC] merging clusters 1 and 2 (topic_sim=0.810 >= 0.80)\n",
      "[MERGE-TOPIC] merging clusters 5 and 8 (topic_sim=0.829 >= 0.80)\n",
      "[MERGE-TOPIC] merging clusters 19 and 20 (topic_sim=0.831 >= 0.80)\n",
      "\n",
      "[Topic-aware merge] Final communities after merging by topic similarity:\n",
      "  merged community 0: ['Cresci2015-FSF - 1127547493', 'Cresci2015-FSF - 1174331750', 'Cresci2015-TWT - 176096243', 'Cresci2015-TWT - 72186699']\n",
      "  merged community 1: ['Cresci2015-FSF - 1175035327', 'Cresci2015-INT - 252647855', 'Cresci2015-INT - 349392750', 'Cresci2015-INT - 370098498']\n",
      "  merged community 2: ['Cresci2015-FSF - 1175854238']\n",
      "  merged community 3: ['Cresci2015-TWT - 17656600', 'Cresci2017-SocialSpambots-3 - 335401278', 'Cresci2017-SocialSpambots-3 - 456934703', 'Cresci2017-SocialSpambots-3 - 531145445', 'Cresci2017-SocialSpambots-3 - 531163502', 'JournalistAttackBrianKrebs - 45779735', 'JournalistAttackBrianKrebs - 99319234', 'StarWarsBotnet - 1580338447']\n",
      "  merged community 4: ['Cresci2015-TWT - 91781300', 'Cresci2017-TraditionalSpambots-1 - 104896954']\n",
      "  merged community 5: ['Cresci2017-SocialSpambots-1 - 1536537319']\n",
      "  merged community 6: ['Cresci2017-SocialSpambots-1 - 2267349139', 'Cresci2017-SocialSpambots-1 - 2274254095', 'Cresci2017-SocialSpambots-1 - 467182961']\n",
      "  merged community 7: ['Cresci2017-TraditionalSpambots-1 - 14596967']\n",
      "  merged community 8: ['Cresci2017-TraditionalSpambots-1 - 52996113']\n",
      "  merged community 9: ['Cresci2017-TraditionalSpambots-1 - 69201180']\n",
      "  merged community 10: ['Cresci2017-TraditionalSpambots-3 - 2766168558', 'Cresci2017-TraditionalSpambots-3 - 3231874357']\n",
      "  merged community 11: ['Cresci2017-TraditionalSpambots-3 - 3231840636']\n",
      "  merged community 12: ['Cresci2017-TraditionalSpambots-3 - 3237288540']\n",
      "  merged community 13: ['Cresci2017-TraditionalSpambots-4 - 38052972']\n",
      "  merged community 14: ['Cresci2017-TraditionalSpambots-4 - 51949191', 'Cresci2017-TraditionalSpambots-4 - 72165407', 'Cresci2017-TraditionalSpambots-4 - 72208069']\n",
      "  merged community 15: ['JournalistAttackBrianKrebs - 52266057']\n",
      "  merged community 16: ['JournalistAttackBrianKrebs - 78664072']\n",
      "  merged community 17: ['StarWarsBotnet - 1553296508', 'StarWarsBotnet - 1561895083', 'StarWarsBotnet - 1569742572']\n",
      "\n",
      "[Cluster 0] Cresci2015 Bot Cluster\n",
      "  signature: frequent use of emojis; casual punctuation (e.g., ellipses, exclamation marks); informal language and slang; use of non-English characters; frequent use of hashtags; repetitive phrases and expressions; random capitalization; short, fragmented sentences; use of quotes from popular culture; frequent use of rhetorical questions\n",
      "  summary: These accounts exhibit a highly informal and casual writing style characterized by the frequent use of emojis and non-standard punctuation. The presence of slang, informal language, and a tendency to use quotes from popular culture further unites them stylistically. Additionally, the use of non-English characters and hashtags adds to their distinctive, playful tone.\n",
      "\n",
      "[Cluster 1] Cresci2015 Bot Cluster\n",
      "  signature: frequent use of ellipses; casual language with slang; repeated phrases; use of emojis and informal punctuation; frequent mentions of Mumbai and Bollywood; use of lowercase for proper nouns; exclamation marks for emphasis; informal greetings and farewells\n",
      "  summary: These accounts share a distinctive informal writing style characterized by casual language, frequent use of ellipses, and a tendency to repeat phrases. They often include informal greetings and exclamations, creating a conversational tone that suggests a close-knit community focused on personal interactions.\n",
      "\n",
      "[Cluster 2] Glaminar Groupies\n",
      "  signature: excessive punctuation; informal language; frequent use of emojis; lowercase casing; slang and abbreviations; casual tone; repetitive phrases; hashtag usage\n",
      "  summary: These accounts share a distinctive informal writing style characterized by excessive punctuation and the use of slang and abbreviations. The frequent use of emojis and lowercase casing further emphasizes their casual tone, creating a cohesive group of bot accounts that engage in similar stylistic choices.\n",
      "\n",
      "[Cluster 3] Cresci Social Spambots\n",
      "  signature: frequent use of ellipses; casual and conversational tone; excessive punctuation (!!!, ???); use of hashtags and emojis; frequent references to pop culture; short, fragmented sentences; use of quotes and proverbs; repetitive sentence structures\n",
      "  summary: These accounts share a distinctive casual and conversational writing style, often using excessive punctuation and emojis to convey excitement or emotion. They frequently reference pop culture and employ short, fragmented sentences, creating a sense of informality and spontaneity in their posts.\n",
      "\n",
      "[Cluster 4] Bot Cluster A\n",
      "  signature: excessive use of 'oie'; frequent use of 'kk' and 'kkkk'; random casing variations; heavy use of emojis and symbols; repetitive character strings; informal language and slang; frequent use of 'bom dia'; use of non-standard punctuation\n",
      "  summary: These accounts exhibit a highly informal and playful writing style characterized by excessive repetition of characters, frequent use of emojis, and a casual tone. The use of specific phrases like 'oie' and 'kk' creates a distinct signature that ties them together, despite the varied content.\n",
      "\n",
      "[Cluster 5] Social Spambots Cluster\n",
      "  signature: frequent emoji usage; irregular casing; excessive punctuation; repetitive phrases; hashtag and mention usage; informal language; short, fragmented sentences; use of slang\n",
      "  summary: These accounts share a distinctive writing style characterized by the frequent use of emojis and irregular casing. Their posts often contain excessive punctuation and repetitive phrases, creating a casual and informal tone that is typical of social media spambot behavior.\n",
      "\n",
      "[Cluster 6] Cresci2017-SocialSpambots\n",
      "  signature: frequent use of ellipses and exclamation marks; informal and humorous tone; use of Italian slang and colloquialisms; repeated phrases for emphasis; playful formatting quirks (e.g., unusual spacing, casing); frequent use of rhetorical questions; mix of English and Italian phrases; use of emojis and excessive punctuation\n",
      "  summary: These accounts share a distinctive informal and humorous writing style characterized by playful language and frequent use of ellipses and exclamation marks. They often employ Italian slang and rhetorical questions, creating a casual and engaging tone that is consistent across the samples.\n",
      "\n",
      "[Cluster 7] Traditional Spambots\n",
      "  signature: repetitive phrasing; informal tone; frequent questions; use of 'check this out'; year references; casual punctuation; no emojis or hashtags\n",
      "  summary: These accounts exhibit a highly repetitive writing style, often using similar phrases and questions to engage the reader. The informal tone and consistent use of specific calls to action, such as 'check this out', create a distinct stylistic fingerprint that links them together.\n",
      "\n",
      "[Cluster 8] Traditional Spambots\n",
      "  signature: frequent use of 'plz rt'; casual casing (e.g., 'i’m', 'you probably haven’t'); exclamation marks for emphasis; repetitive structure in posts; use of numbers in titles; informal language; short, punchy sentences; use of questions to engage; frequent mentions of money-making; use of phrases like 'check out my' and 'how to'\n",
      "  summary: These accounts share a distinctive style characterized by informal language and a repetitive structure that emphasizes engagement through questions and calls to action. The frequent use of 'plz rt' and casual casing contributes to a cohesive identity focused on promoting online marketing strategies and money-making tips.\n",
      "\n",
      "[Cluster 9] Traditional Spambots\n",
      "  signature: frequent use of brackets; lowercase titles; ampersand usage; list format with colons; repetitive structure; audiobook mentions; long, descriptive phrases\n",
      "  summary: These accounts share a distinctive style characterized by the frequent use of brackets to denote categories and a consistent format that lists titles followed by their descriptions. The use of lowercase for titles and the inclusion of 'audiobook' in nearly every entry further solidifies their stylistic similarity.\n",
      "\n",
      "[Cluster 10] Traditional Spambots\n",
      "  signature: frequent use of colons; long, unpunctuated sentences; inconsistent casing; use of dashes for separation; repetitive job titles; lack of emojis or hashtags; use of company names and job descriptions; frequent mentions of job locations\n",
      "  summary: These accounts exhibit a similar writing style characterized by lengthy, unpunctuated sentences and a heavy reliance on colons to introduce job descriptions. The use of dashes for separation and inconsistent casing further unifies their stylistic approach, while the content focuses on job postings and company information.\n",
      "\n",
      "[Cluster 11] Traditional Spambots\n",
      "  signature: frequent use of question marks; excessive use of emojis; repeated phrases and structures; use of promotional language; inconsistent casing; frequent use of 'learn more'; hashtag usage for days of the week; use of 'see details' and 'check out'\n",
      "  summary: These accounts share a distinctive style characterized by excessive emoji usage and promotional language. They often employ repetitive phrases and inconsistent casing, which creates a recognizable pattern typical of traditional spambot behavior.\n",
      "\n",
      "[Cluster 12] Traditional Spambots\n",
      "  signature: frequent use of 'news latest'; repetitive phrases like 'it jobs'; lack of punctuation variety; minimal capitalization; use of question marks for emphasis; short, fragmented sentences; no emojis or hashtags; focus on job-related content\n",
      "  summary: These accounts share a distinctive writing style characterized by repetitive phrases and a lack of punctuation variety. The frequent use of 'news latest' and 'it jobs' creates a uniformity in their messaging, while the minimal capitalization and fragmented sentences contribute to a simplistic and robotic tone.\n",
      "\n",
      "[Cluster 13] Louisville Job Spambots\n",
      "  signature: frequent use of 'louisville, ky'; repetitive job posting structure; use of 'click to apply'; multiple job titles listed; consistent use of 'hiring careerarc'; exclamation marks for emphasis; short, direct sentences; use of parentheses for additional information; frequent use of 'we're hiring!'\n",
      "  summary: These accounts share a distinct writing style characterized by repetitive job postings focused on opportunities in Louisville, KY. The use of specific phrases like 'click to apply' and 'hiring careerarc' creates a uniformity in their messaging, emphasizing urgency and availability of jobs.\n",
      "\n",
      "[Cluster 14] Traditional Spambots\n",
      "  signature: frequent use of 'customerservice'; repetitive job opening phrases; use of location-specific job details; consistent use of 'hiring careerarc'; excessive use of question prompts; lack of varied punctuation; capitalization of job titles; short, fragmented sentences; use of 'click to apply' calls to action; repeated phrases across accounts\n",
      "  summary: These accounts exhibit a highly repetitive writing style focused on job advertisements, primarily in the customer service sector. They share common phrases, formatting quirks, and a consistent structure that emphasizes job openings and calls to action, indicating a coordinated effort to promote job listings.\n",
      "\n",
      "[Cluster 15] Electronic Music Bots\n",
      "  signature: frequent use of emojis; repetitive phrases; casual punctuation; hashtag usage; informal casing; use of ellipses; direct address to audience; inspirational quotes; mention of music collaborations; frequent references to personal feelings\n",
      "  summary: These accounts share a distinctive style characterized by casual punctuation and frequent use of emojis, which creates an informal and engaging tone. The repetitive phrases and inspirational quotes suggest a focus on emotional connection, particularly in relation to music and personal experiences.\n",
      "\n",
      "[Cluster 16] Traveling Bot Accounts\n",
      "  signature: frequent use of exclamation marks; casual punctuation (ellipses, question marks); use of emojis to express emotions; informal casing (lowercase for beginnings); repetitive sentence structures; location mentions with '@'; short, fragmented sentences; use of conversational phrases\n",
      "  summary: These accounts share a distinctive informal writing style characterized by casual punctuation and frequent use of emojis. The use of location mentions with '@' and repetitive sentence structures creates a cohesive narrative of travel experiences.\n",
      "\n",
      "[Cluster 17] StarWarsBotnet\n",
      "  signature: frequent use of lowercase for character names; dialogue formatted with quotes; use of ellipses for pauses; repetitive sentence structures; occasional use of technical jargon; descriptive action sequences; interspersed character thoughts; minimal punctuation variation\n",
      "  summary: These accounts share a distinctive writing style characterized by the frequent use of lowercase for character names and a consistent format for dialogue. The narrative features repetitive sentence structures and descriptive action sequences, creating a cohesive stylistic identity across the cluster.\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    set_random_seed(42)\n",
    "\n",
    "    if not accounts:\n",
    "        raise SystemExit(\"Please populate the 'accounts' dict with your bots and tweets.\")\n",
    "\n",
    "    # A) Vectorize (stylometry)\n",
    "    vec, acc_vecs, chunks_by_acc, n_features = fit_vectorizer(accounts, ngram_range=(3, 5), max_features=120_000)\n",
    "\n",
    "    # B) AV similarity matrix (scaled with candidate prefilter + parallel + early stop)\n",
    "    ids = list(accounts.keys())\n",
    "    candidates = build_candidate_pairs(ids, acc_vecs, max_candidates=50, min_cos=0.05)\n",
    "    print(f\"[AV] candidate pairs: {len(candidates)} (out of {len(ids)*(len(ids)-1)//2})\")\n",
    "    S_av = build_similarity_matrix_AV(\n",
    "        ids, acc_vecs, n_features,\n",
    "        n_trials=200, feat_frac=0.40, bg_per_trial=30, seed=42,\n",
    "        candidate_pairs=candidates, n_jobs=max(1, os.cpu_count() // 2),\n",
    "        early_stop=True, eps=0.02, delta=1e-3, n_masks=256\n",
    "    )\n",
    "\n",
    "    S_embed = None; embedder = None\n",
    "    if API_KEY:\n",
    "        embedder = OpenAIEmbeddings(model=\"text-embedding-3-small\")   \n",
    "        acc_emb = build_account_embeddings(chunks_by_acc, embedder)\n",
    "        S_embed = cosine_matrix_from_embeddings(ids, acc_emb)\n",
    "    else:\n",
    "        print(\"[INFO] Embeddings disabled (no OPENAI_API_KEY). Using AV signal only.\")\n",
    "\n",
    "    # D) Fusion & community detection (Leiden by default; Louvain fallback)\n",
    "    alpha = 0.5\n",
    "    S_fused = fuse_scores(S_av, S_embed, alpha=alpha)\n",
    "    community_method = \"leiden\" if globals().get(\"HAS_LEIDEN\", False) else \"louvain\"\n",
    "    best = sweep_tau(S_fused, ids, taus=np.linspace(0.58, 0.72, 7), resolution=1.0, method=community_method)\n",
    "    print(f\"[{community_method.capitalize()}] best_tau={best['tau']:.2f}  modularity Q={best['Q']:.3f}\")\n",
    "    for cid, members in (best[\"clusters\"] or {}).items():\n",
    "        print(f\"  community {cid}: {members}\")\n",
    "\n",
    "    # E) Topic-aware merging\n",
    "    merged_clusters = best[\"clusters\"] or {}\n",
    "    if embedder is not None and merged_clusters:\n",
    "        merged_clusters = merge_clusters_by_topic(\n",
    "            merged_clusters,\n",
    "            accounts_texts=accounts,\n",
    "            embedder=embedder,\n",
    "            topic_tau=0.8,      # if two clusters' topical cosine ≥ 0.80, merge them\n",
    "            verbose=True\n",
    "        )\n",
    "        if merged_clusters != (best[\"clusters\"] or {}):\n",
    "            print(\"\\n[Topic-aware merge] Final communities after merging by topic similarity:\")\n",
    "            for cid, members in merged_clusters.items():\n",
    "                print(f\"  merged community {cid}: {members}\")\n",
    "\n",
    "    labels = {}\n",
    "    if API_KEY and merged_clusters:\n",
    "        labels = llm_label_clusters(merged_clusters, accounts, llm_model=\"gpt-4o-mini\", temperature=0.0)\n",
    "        for cid, lab in labels.items():\n",
    "            print(f\"\\n[Cluster {cid}] {lab.cluster_name}\")\n",
    "            print(\"  signature:\", \"; \".join(lab.style_signature))\n",
    "            print(\"  summary:\", lab.cohesion_summary)\n",
    "            if lab.notable_outliers:\n",
    "                print(\"  outliers:\", \", \".join(lab.notable_outliers)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
