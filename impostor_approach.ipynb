{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757ed941",
   "metadata": {},
   "source": [
    "## Botnet discovery with unknown K:\n",
    "All accounts are treated uniformly; background = all *other* accounts (leave-two-out).\n",
    "We compute Impostors-style AV similarities and detect communities via Louvain modularity.\n",
    "\n",
    "* https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html\n",
    "* https://ceur-ws.org/Vol-1179/CLEF2013wn-PAN-Seidman2013.pdf\n",
    "* https://arxiv.org/abs/1810.08473\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f84753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, random, numpy as np, pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c676a09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\felip\\AppData\\Local\\Temp\\ipykernel_23972\\1746022432.py:1: DtypeWarning: Columns (16) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_full_enriched.csv\")\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv(\"./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_full_enriched.csv\")\n",
    "\n",
    "def get_tweets_from_user(user_id: int) -> List[str]:\n",
    "    return df.loc[(df[\"user_id\"] == user_id) & (df[\"is_retweet\"] == False), \"tweet_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "accounts: Dict[str, List[str]] = {\n",
    "    \"acc_A\": get_tweets_from_user(21479334), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_B\": get_tweets_from_user(1544624887), # starwars\n",
    "    \"acc_C\": get_tweets_from_user(1585571839), # starwars\n",
    "    \"acc_D\": get_tweets_from_user(21674676), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_E\": get_tweets_from_user(1546564405), # starwars\n",
    "    \"acc_F\": get_tweets_from_user(1587209611), # starwars\n",
    "    \"acc_G\": get_tweets_from_user(1552496288), # starwars\n",
    "    \"acc_H\": get_tweets_from_user(22728442), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_I\": get_tweets_from_user(22759829), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_J\": get_tweets_from_user(22578323), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_K\": get_tweets_from_user(74793689), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_L\": get_tweets_from_user(38095383), # cresci17 - Traditional Spambots - 4\n",
    "    \"acc_M\": get_tweets_from_user(1557300746), # starwars\n",
    "    \"acc_N\": get_tweets_from_user(1559362050), # starwars\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4056e9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "user_id\n",
       "725325656             1103\n",
       "1401156210            1073\n",
       "473212705             1021\n",
       "45571859               975\n",
       "500713601              935\n",
       "82209264               929\n",
       "57019818               928\n",
       "19512396               928\n",
       "276803336              893\n",
       "541492320              892\n",
       "3273475567             875\n",
       "111094423              851\n",
       "2423506384             787\n",
       "1655215466             766\n",
       "4645681828             734\n",
       "105222131              730\n",
       "54260807               700\n",
       "851226858280505345     683\n",
       "227184175              669\n",
       "226040131              611\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.loc[df[\"carpeta_origen\"].eq(\"JournalistAttackBrianKrebs\"), \"user_id\"].value_counts().iloc[100:120]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "af8bf03d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "carpeta_origen\n",
       "Cresci2017-SocialSpambots-3         1408728\n",
       "Cresci2017-SocialSpambots-1         1269264\n",
       "Cresci2017-TraditionalSpambots-3     791735\n",
       "JournalistAttackBrianKrebs           304017\n",
       "Cresci2017-TraditionalSpambots-1     185533\n",
       "Cresci2015-TWT                       156009\n",
       "Cresci2017-TraditionalSpambots-4      60600\n",
       "StarWarsBotnet                        26971\n",
       "Cresci2015-INT                         3315\n",
       "Cresci2015-FSF                          695\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"carpeta_origen\"].value_counts()\n",
    "#print(df[df[\"carpeta_origen\"] == \"Cresci2015-INT\"][\"user_id\"].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea7cec",
   "metadata": {},
   "source": [
    "## Normalization & chunking\n",
    "\n",
    "Normalization removes URL/mention noise and standardizes casing/spacing.\n",
    "Chunking creates *equal-sized* text blocks so similarity isn't dominated by account length.\n",
    "This is common in AV to stabilize short/noisy texts and prevent length/template bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619890de",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE        = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_RE    = re.compile(r'@\\w+')\n",
    "WHITESPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "def normalize_tweet(t: str, keep_hashtags: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, strip URLs and @mentions, collapse whitespace.\n",
    "    Keep hashtags by removing the '#' (style usage can be informative).\n",
    "    \"\"\"\n",
    "    t = t.lower()\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "    if keep_hashtags:\n",
    "        t = t.replace(\"#\", \"\")\n",
    "    t = t.replace(\"&amp;\", \"&\")\n",
    "    t = WHITESPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def unique_norm_texts(texts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Deduplicate normalized tweets so repeated boilerplate doesn't overweight the model.\n",
    "    (We don't hardcode templates; we just avoid exact duplicate rows after normalization.)\n",
    "    \"\"\"\n",
    "    seen, uniq = set(), []\n",
    "    for s in texts:\n",
    "        n = normalize_tweet(s)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); uniq.append(n)\n",
    "    return uniq\n",
    "\n",
    "def make_char_chunks(texts: List[str], target_chars: int = 5000, max_chunks: int = 12) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build ~equal-sized character chunks (≈target_chars).\n",
    "    This balances accounts with many vs. few tweets and yields multiple comparable samples per account.\n",
    "    \"\"\"\n",
    "    msgs = unique_norm_texts(texts)\n",
    "    if not msgs:\n",
    "        return []\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for m in msgs:\n",
    "        cur.append(m); cur_len += len(m) + 1\n",
    "        if cur_len >= target_chars:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_len = [], 0\n",
    "            if len(chunks) >= max_chunks:\n",
    "                break\n",
    "    if cur and len(chunks) < max_chunks:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks if chunks else [\" \".join(msgs)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e4e63",
   "metadata": {},
   "source": [
    "## Vectorization (TF–IDF over char n-grams)\n",
    "Char n-grams are strong, topic-light style signals for short/informal texts (tweets),\n",
    "widely used in AV & PAN tasks. We also guard TF–IDF with min_df/max_df to drop too-rare or too-common grams.\n",
    "scikit-learn semantics: min_df/max_df can be counts or proportions; we auto-tune to avoid conflicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceddeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(\n",
    "    acc_texts: Dict[str, List[str]],\n",
    "    ngram_range: Tuple[int,int]=(3,5),\n",
    "    max_features: int = 200_000,\n",
    "    min_df=None, max_df=None, sublinear_tf: bool = True,\n",
    "):\n",
    "    docs, ids = [], []\n",
    "    per_acc_counts = {}\n",
    "    for aid, texts in acc_texts.items():\n",
    "        chunks = make_char_chunks(texts, target_chars=5000, max_chunks=12)\n",
    "        per_acc_counts[aid] = len(chunks)\n",
    "        for ch in chunks:\n",
    "            docs.append(ch); ids.append(aid)\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < 2:\n",
    "        raise ValueError(f\"Too few documents/chunks ({n_docs}). Increase data or lower target_chars.\")\n",
    "\n",
    "    # Auto-safe DF thresholds to respect sklearn's constraint: max_df_docs >= min_df_docs\n",
    "    user_min, user_max = min_df, max_df\n",
    "    if min_df is None: min_df = max(1, int(0.01 * n_docs)) \n",
    "    if max_df is None: max_df = 0.90                         \n",
    "\n",
    "    max_df_docs = int(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    min_df_docs = int(min_df) if isinstance(min_df, int) else int(min_df * n_docs)\n",
    "\n",
    "    if max_df_docs < min_df_docs:\n",
    "        min_df_docs = max(1, max_df_docs)\n",
    "        if isinstance(user_min, float): min_df = max(user_min, min_df_docs / n_docs)\n",
    "        else:                           min_df = min_df_docs\n",
    "\n",
    "    max_df_docs = int(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    min_df_docs = int(min_df) if isinstance(min_df, int) else int(min_df * n_docs)\n",
    "    if max_df_docs < min_df_docs:\n",
    "        if isinstance(max_df, float): max_df = min(0.99, (min_df_docs + 1) / n_docs)\n",
    "        else:                         max_df = min_df_docs + 1\n",
    "\n",
    "    print(f\"[fit_vectorizer] n_docs={n_docs} chunks_per_account={per_acc_counts}  min_df={min_df}  max_df={max_df}\")\n",
    "\n",
    "    vec = TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=ngram_range,\n",
    "        min_df=min_df, max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        norm=\"l2\", sublinear_tf=sublinear_tf, \n",
    "    )\n",
    "    X = vec.fit_transform(docs)\n",
    "\n",
    "    account_vecs = {}\n",
    "    for i, aid in enumerate(ids):\n",
    "        account_vecs.setdefault(aid, []).append(X[i])\n",
    "\n",
    "    n_features = len(vec.get_feature_names_out())\n",
    "    return vec, account_vecs, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9de893",
   "metadata": {},
   "source": [
    "## Impostors-style AV (chunked + symmetric)\n",
    "\n",
    "The Impostors method answers \"same author?\" by asking if A–B similarity\n",
    "repeatedly beats A–(random background) across many random feature subspaces.\n",
    "We run it symmetrically (A→B and B→A) to reduce anchor bias; this matches\n",
    "good AV practice on short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663c60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def _random_feature_mask(n_features: int, keep_frac: float) -> np.ndarray:\n",
    "    \"\"\"Sample a random subset of features (columns) to form a subspace per trial.\"\"\"\n",
    "    k = max(1, int(n_features * keep_frac))\n",
    "    mask = np.zeros(n_features, dtype=bool)\n",
    "    idx = np.random.choice(n_features, size=k, replace=False)\n",
    "    mask[idx] = True\n",
    "    return mask\n",
    "\n",
    "def _masked_cosine(a, b, feat_mask: np.ndarray) -> float:\n",
    "    \"\"\"Cosine similarity in the chosen subspace (1 x d sparse rows).\"\"\"\n",
    "    return float(cosine_similarity(a[:, feat_mask], b[:, feat_mask])[0, 0])\n",
    "\n",
    "def _pick_chunk(vecs: List) -> np.ndarray:\n",
    "    \"\"\"Uniformly sample one chunk vector for this account (varies across trials).\"\"\"\n",
    "    if not vecs: raise ValueError(\"Empty chunk list for account.\")\n",
    "    return vecs[np.random.randint(len(vecs))]\n",
    "\n",
    "def impostors_score_chunks(\n",
    "    A_chunks: List, B_chunks: List, background_accounts: List[List],\n",
    "    n_features: int, n_trials: int = 400, feat_frac: float = 0.5, bg_accounts_per_trial: int = 25\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fraction of trials where sim(A,B) > max sim(A, any sampled background account),\n",
    "    using a fresh random feature subspace each trial.\n",
    "    \"\"\"\n",
    "    poolN = len(background_accounts)\n",
    "    if poolN == 0:\n",
    "        return 0.0\n",
    "    wins = 0\n",
    "    k = min(bg_accounts_per_trial, poolN)\n",
    "    for _ in range(n_trials):\n",
    "        mask = _random_feature_mask(n_features, feat_frac)\n",
    "        vA = _pick_chunk(A_chunks)\n",
    "        vB = _pick_chunk(B_chunks)\n",
    "        sim_AB = _masked_cosine(vA, vB, mask)\n",
    "        idxs = np.random.choice(poolN, size=k, replace=False)\n",
    "        sims_AI = [_masked_cosine(vA, _pick_chunk(background_accounts[j]), mask) for j in idxs]\n",
    "        if sim_AB > max(sims_AI, default=-1.0):\n",
    "            wins += 1\n",
    "    return wins / n_trials\n",
    "\n",
    "def symmetric_impostors_score(\n",
    "    A_chunks: List, B_chunks: List, background_accounts: List[List],\n",
    "    n_features: int, n_trials: int = 400, feat_frac: float = 0.5, bg_accounts_per_trial: int = 25\n",
    ") -> float:\n",
    "    \"\"\"Average of A→B vs background and B→A vs background.\"\"\"\n",
    "    s1 = impostors_score_chunks(A_chunks, B_chunks, background_accounts,\n",
    "                                n_features, n_trials, feat_frac, bg_accounts_per_trial)\n",
    "    s2 = impostors_score_chunks(B_chunks, A_chunks, background_accounts,\n",
    "                                n_features, n_trials, feat_frac, bg_accounts_per_trial)\n",
    "    return 0.5 * (s1 + s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a281d8",
   "metadata": {},
   "source": [
    "## Build all-pairs similarity (leave-two-out background)\n",
    "\n",
    "We treat *all* accounts uniformly. For each pair (A,B),\n",
    "the background is \"all other accounts\" (leave-two-out).\n",
    "This removes the need for a separate impostor list and keeps the setting fully unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd4dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(\n",
    "    ids: List[str],\n",
    "    account_chunk_vectors: Dict[str, List],  # id -> [chunk vectors]\n",
    "    n_features: int,\n",
    "    n_trials: int = 800, feat_frac: float = 0.45, bg_accounts_per_trial: int = 40, seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    np.random.seed(seed)\n",
    "    n = len(ids)\n",
    "    S = np.eye(n, dtype=float) \n",
    "\n",
    "    chunks = {bid: account_chunk_vectors[bid] for bid in ids}\n",
    "\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        A_id, B_id = ids[i], ids[j]\n",
    "        A_chunks, B_chunks = chunks[A_id], chunks[B_id]\n",
    "\n",
    "        # Background = all other accounts (leave two out)\n",
    "        background = [chunks[k] for k in ids if k not in (A_id, B_id)]\n",
    "\n",
    "        s = symmetric_impostors_score(\n",
    "            A_chunks, B_chunks, background, n_features,\n",
    "            n_trials=n_trials, feat_frac=feat_frac, bg_accounts_per_trial=bg_accounts_per_trial\n",
    "        )\n",
    "        S[i, j] = S[j, i] = s\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb2c18",
   "metadata": {},
   "source": [
    "## Community detection (unknown K) via Louvain\n",
    "Turn S into a graph (edge if similarity >= tau) and find communities that maximize modularity.\n",
    "We sweep tau and keep the partition with the best modularity Q (NetworkX provides both).\n",
    "`next implementation: Leiden`\n",
    "Note: Leiden improves Louvain (connectedness & quality); consider it for larger graphs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a762411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_louvain(S: np.ndarray, ids: List[str], tau: float = 0.65, resolution: float = 1.0, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Threshold S at tau, build weighted graph, detect communities with Louvain.\n",
    "    Returns (clusters_dict, modularity_Q).\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(ids)\n",
    "    for i in range(len(ids)):\n",
    "        for j in range(i+1, len(ids)):\n",
    "            w = float(S[i, j])\n",
    "            if w >= tau:\n",
    "                G.add_edge(ids[i], ids[j], weight=w)\n",
    "\n",
    "    if G.number_of_edges() == 0:\n",
    "        return {}, 0.0\n",
    "\n",
    "    # Louvain communities (greedy modularity optimization) — returns sets of nodes\n",
    "    comms = louvain_communities(G, weight=\"weight\", resolution=resolution, seed=seed)  \n",
    "\n",
    "    # Modularity score Q = quality of the partition (higher = clearer community structure)\n",
    "    from networkx.algorithms.community import modularity\n",
    "    Q = modularity(G, comms, weight=\"weight\") \n",
    "\n",
    "    clusters = {i: sorted(list(c)) for i, c in enumerate(comms)}\n",
    "    return clusters, Q\n",
    "\n",
    "def sweep_tau_for_best_modularity(S: np.ndarray, ids: List[str], taus=np.linspace(0.58, 0.76, 5), resolution: float = 1.0):\n",
    "    \"\"\"\n",
    "    Try several thresholds and keep the partition with highest modularity Q.\n",
    "    This removes the need to guess a single tau value up front.\n",
    "    \"\"\"\n",
    "    best = {\"tau\": None, \"Q\": -1, \"clusters\": None}\n",
    "    for tau in taus:\n",
    "        clusters, Q = cluster_with_louvain(S, ids, tau=tau, resolution=resolution)\n",
    "        if Q is not None and Q > best[\"Q\"]:\n",
    "            best = {\"tau\": tau, \"Q\": Q, \"clusters\": clusters}\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f724b2",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_vectorizer] n_docs=14 chunks_per_account={'acc_A': 1, 'acc_B': 1, 'acc_C': 1, 'acc_D': 1, 'acc_E': 1, 'acc_F': 1, 'acc_G': 1, 'acc_H': 1, 'acc_I': 1, 'acc_J': 1, 'acc_K': 1, 'acc_L': 1, 'acc_M': 1, 'acc_N': 1}  min_df=1  max_df=0.9\n",
      "IDs: ['acc_A', 'acc_B', 'acc_C', 'acc_D', 'acc_E', 'acc_F', 'acc_G', 'acc_H', 'acc_I', 'acc_J', 'acc_K', 'acc_L', 'acc_M', 'acc_N']\n",
      "Similarity matrix (rounded):\n",
      " [[1.    0.    0.    1.    0.    0.    0.    0.086 0.    0.033 0.    0.471\n",
      "  0.    0.   ]\n",
      " [0.    1.    0.065 0.    0.    0.001 0.421 0.    0.    0.    0.    0.\n",
      "  0.16  0.   ]\n",
      " [0.    0.065 1.    0.    0.003 0.001 0.011 0.    0.    0.    0.    0.\n",
      "  0.594 0.002]\n",
      " [1.    0.    0.    1.    0.    0.    0.    0.    0.059 0.    0.017 0.\n",
      "  0.    0.   ]\n",
      " [0.    0.    0.003 0.    1.    0.003 0.    0.    0.    0.    0.    0.\n",
      "  0.001 0.498]\n",
      " [0.    0.001 0.001 0.    0.003 1.    0.227 0.    0.    0.    0.    0.\n",
      "  0.236 0.063]\n",
      " [0.    0.421 0.011 0.    0.    0.227 1.    0.    0.    0.    0.    0.\n",
      "  0.117 0.178]\n",
      " [0.086 0.    0.    0.    0.    0.    0.    1.    0.    0.014 0.    0.413\n",
      "  0.    0.   ]\n",
      " [0.    0.    0.    0.059 0.    0.    0.    0.    1.    0.036 0.004 0.47\n",
      "  0.    0.   ]\n",
      " [0.033 0.    0.    0.    0.    0.    0.    0.014 0.036 1.    0.474 0.417\n",
      "  0.    0.   ]\n",
      " [0.    0.    0.    0.017 0.    0.    0.    0.    0.004 0.474 1.    0.011\n",
      "  0.    0.   ]\n",
      " [0.471 0.    0.    0.    0.    0.    0.    0.413 0.47  0.417 0.011 1.\n",
      "  0.    0.   ]\n",
      " [0.    0.16  0.594 0.    0.001 0.236 0.117 0.    0.    0.    0.    0.\n",
      "  1.    0.915]\n",
      " [0.    0.    0.002 0.    0.498 0.063 0.178 0.    0.    0.    0.    0.\n",
      "  0.915 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "vec, account_chunk_vectors, n_features = fit_vectorizer(\n",
    "    accounts, ngram_range=(3,5), min_df=None, max_df=None, sublinear_tf=True, max_features=120_000\n",
    ")\n",
    "\n",
    "ids_to_cluster = list(accounts.keys())\n",
    "S = build_similarity_matrix(\n",
    "    ids_to_cluster, account_chunk_vectors, n_features,\n",
    "    n_trials=150, feat_frac=0.40, bg_accounts_per_trial=25, seed=42\n",
    ")\n",
    "\n",
    "print(\"IDs:\", ids_to_cluster)\n",
    "#print(\"Similarity matrix (rounded):\\n\", np.round(S, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d71c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Louvain] best_tau=0.62  modularity(Q)=0.499\n",
      "  Botnet 0: ['acc_A', 'acc_D']\n",
      "  Botnet 1: ['acc_B']\n",
      "  Botnet 2: ['acc_C']\n",
      "  Botnet 3: ['acc_E']\n",
      "  Botnet 4: ['acc_F']\n",
      "  Botnet 5: ['acc_G']\n",
      "  Botnet 6: ['acc_H']\n",
      "  Botnet 7: ['acc_I']\n",
      "  Botnet 8: ['acc_J']\n",
      "  Botnet 9: ['acc_K']\n",
      "  Botnet 10: ['acc_L']\n",
      "  Botnet 11: ['acc_M', 'acc_N']\n"
     ]
    }
   ],
   "source": [
    "best = sweep_tau_for_best_modularity(S, ids_to_cluster, taus=np.linspace(0.58, 0.76, 5), resolution=1.0)\n",
    "print(f\"\\n[Louvain] best_tau={best['tau']:.2f}  modularity(Q)={best['Q']:.3f}\")\n",
    "for cid, members in (best[\"clusters\"] or {}).items():\n",
    "    print(f\"  Botnet {cid}: {members}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
