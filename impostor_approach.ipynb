{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "757ed941",
   "metadata": {},
   "source": [
    "## Botnet discovery with unknown K:\n",
    "All accounts are treated uniformly; background = all *other* accounts (leave-two-out).\n",
    "We compute Impostors-style AV similarities and detect communities via Louvain modularity.\n",
    "\n",
    "* https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.community.louvain.louvain_communities.html?utm_source=chatgpt.com\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f84753",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, random, numpy as np, pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import networkx as nx\n",
    "from networkx.algorithms.community import louvain_communities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c676a09f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./PhDBotnetsDB/User_Tweet/_outputs/user_tweets_sample_enriched.csv\")\n",
    "\n",
    "def get_tweets_from_user(user_id: int) -> List[str]:\n",
    "    return df.loc[(df[\"user_id\"] == user_id) & (df[\"is_retweet\"] == False), \"tweet_text\"].dropna().astype(str).tolist()\n",
    "\n",
    "accounts: Dict[str, List[str]] = {\n",
    "    \"acc_A\": get_tweets_from_user(21479334), # cresci17\n",
    "    \"acc_B\": get_tweets_from_user(1544624887), # starwars\n",
    "    \"acc_C\": get_tweets_from_user(1585571839), # starwars\n",
    "    \"acc_D\": get_tweets_from_user(21674676), # cresci17\n",
    "    \"acc_E\": get_tweets_from_user(1546564405), # starwars\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "af8bf03d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"carpeta_origen\"].value_counts()\n",
    "#df[df[\"carpeta_origen\"] == \"StarWarsBotnet\"][\"user_id\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ea7cec",
   "metadata": {},
   "source": [
    "## Normalization & chunking\n",
    "\n",
    "Normalization removes URL/mention noise and standardizes casing/spacing.\n",
    "Chunking creates *equal-sized* text blocks so similarity isn't dominated by account length.\n",
    "This is common in AV to stabilize short/noisy texts and prevent length/template bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "619890de",
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_RE        = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "MENTION_RE    = re.compile(r'@\\w+')\n",
    "WHITESPACE_RE = re.compile(r'\\s+')\n",
    "\n",
    "def normalize_tweet(t: str, keep_hashtags: bool = True) -> str:\n",
    "    \"\"\"\n",
    "    Lowercase, strip URLs and @mentions, collapse whitespace.\n",
    "    Keep hashtags by removing the '#' (style usage can be informative).\n",
    "    \"\"\"\n",
    "    t = t.lower()\n",
    "    t = URL_RE.sub(\" \", t)\n",
    "    t = MENTION_RE.sub(\" \", t)\n",
    "    if keep_hashtags:\n",
    "        t = t.replace(\"#\", \"\")\n",
    "    t = t.replace(\"&amp;\", \"&\")\n",
    "    t = WHITESPACE_RE.sub(\" \", t).strip()\n",
    "    return t\n",
    "\n",
    "def unique_norm_texts(texts: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Deduplicate normalized tweets so repeated boilerplate doesn't overweight the model.\n",
    "    (We don't hardcode templates; we just avoid exact duplicate rows after normalization.)\n",
    "    \"\"\"\n",
    "    seen, uniq = set(), []\n",
    "    for s in texts:\n",
    "        n = normalize_tweet(s)\n",
    "        if n and n not in seen:\n",
    "            seen.add(n); uniq.append(n)\n",
    "    return uniq\n",
    "\n",
    "def make_char_chunks(texts: List[str], target_chars: int = 5000, max_chunks: int = 12) -> List[str]:\n",
    "    \"\"\"\n",
    "    Build ~equal-sized character chunks (≈target_chars).\n",
    "    This balances accounts with many vs. few tweets and yields multiple comparable samples per account.\n",
    "    \"\"\"\n",
    "    msgs = unique_norm_texts(texts)\n",
    "    if not msgs:\n",
    "        return []\n",
    "    chunks, cur, cur_len = [], [], 0\n",
    "    for m in msgs:\n",
    "        cur.append(m); cur_len += len(m) + 1\n",
    "        if cur_len >= target_chars:\n",
    "            chunks.append(\" \".join(cur))\n",
    "            cur, cur_len = [], 0\n",
    "            if len(chunks) >= max_chunks:\n",
    "                break\n",
    "    if cur and len(chunks) < max_chunks:\n",
    "        chunks.append(\" \".join(cur))\n",
    "    return chunks if chunks else [\" \".join(msgs)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e64e4e63",
   "metadata": {},
   "source": [
    "## Vectorization (TF–IDF over char n-grams)\n",
    "Char n-grams are strong, topic-light style signals for short/informal texts (tweets),\n",
    "widely used in AV & PAN tasks. We also guard TF–IDF with min_df/max_df to drop too-rare or too-common grams.\n",
    "scikit-learn semantics: min_df/max_df can be counts or proportions; we auto-tune to avoid conflicts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ceddeee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_vectorizer(\n",
    "    acc_texts: Dict[str, List[str]],\n",
    "    ngram_range: Tuple[int,int]=(3,5),      # char 3–5 grams are a strong default for tweets\n",
    "    max_features: int = 200_000,\n",
    "    min_df=None, max_df=None, sublinear_tf: bool = True,\n",
    "):\n",
    "    docs, ids = [], []\n",
    "    per_acc_counts = {}\n",
    "    for aid, texts in acc_texts.items():\n",
    "        chunks = make_char_chunks(texts, target_chars=5000, max_chunks=12)\n",
    "        per_acc_counts[aid] = len(chunks)\n",
    "        for ch in chunks:\n",
    "            docs.append(ch); ids.append(aid)\n",
    "\n",
    "    n_docs = len(docs)\n",
    "    if n_docs < 2:\n",
    "        raise ValueError(f\"Too few documents/chunks ({n_docs}). Increase data or lower target_chars.\")\n",
    "\n",
    "    # Auto-safe DF thresholds to respect sklearn's constraint: max_df_docs >= min_df_docs\n",
    "    user_min, user_max = min_df, max_df\n",
    "    if min_df is None: min_df = max(1, int(0.01 * n_docs))  # at least 1 doc or ~1%\n",
    "    if max_df is None: max_df = 0.90                         # drop corpus-ubiquitous grams\n",
    "\n",
    "    max_df_docs = int(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    min_df_docs = int(min_df) if isinstance(min_df, int) else int(min_df * n_docs)\n",
    "    if max_df_docs < min_df_docs:\n",
    "        # Relax min_df first (common quick fix for small corpora)\n",
    "        min_df_docs = max(1, max_df_docs)\n",
    "        if isinstance(user_min, float): min_df = max(user_min, min_df_docs / n_docs)\n",
    "        else:                           min_df = min_df_docs\n",
    "\n",
    "    # Final safety: if rounding still conflicts, lift max_df ever so slightly\n",
    "    max_df_docs = int(max_df * n_docs) if isinstance(max_df, float) else int(max_df)\n",
    "    min_df_docs = int(min_df) if isinstance(min_df, int) else int(min_df * n_docs)\n",
    "    if max_df_docs < min_df_docs:\n",
    "        if isinstance(max_df, float): max_df = min(0.99, (min_df_docs + 1) / n_docs)\n",
    "        else:                         max_df = min_df_docs + 1\n",
    "\n",
    "    print(f\"[fit_vectorizer] n_docs={n_docs} chunks_per_account={per_acc_counts}  min_df={min_df}  max_df={max_df}\")\n",
    "\n",
    "    vec = TfidfVectorizer(\n",
    "        analyzer=\"char\", ngram_range=ngram_range,\n",
    "        min_df=min_df, max_df=max_df,\n",
    "        max_features=max_features,\n",
    "        norm=\"l2\", sublinear_tf=sublinear_tf,  # damp term-frequency spikes\n",
    "    )\n",
    "    X = vec.fit_transform(docs)\n",
    "\n",
    "    # Map each account to its list of chunk vectors (we'll sample from these later)\n",
    "    account_vecs = {}\n",
    "    for i, aid in enumerate(ids):\n",
    "        account_vecs.setdefault(aid, []).append(X[i])\n",
    "\n",
    "    n_features = len(vec.get_feature_names_out())\n",
    "    return vec, account_vecs, n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9de893",
   "metadata": {},
   "source": [
    "## Impostors-style AV (chunked + symmetric)\n",
    "\n",
    "The Impostors method answers \"same author?\" by asking if A–B similarity\n",
    "repeatedly beats A–(random background) across many random feature subspaces.\n",
    "We run it symmetrically (A→B and B→A) to reduce anchor bias; this matches\n",
    "good AV practice on short texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "663c60ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_random_seed(seed=42):\n",
    "    random.seed(seed); np.random.seed(seed)\n",
    "\n",
    "def _random_feature_mask(n_features: int, keep_frac: float) -> np.ndarray:\n",
    "    \"\"\"Sample a random subset of features (columns) to form a subspace per trial.\"\"\"\n",
    "    k = max(1, int(n_features * keep_frac))\n",
    "    mask = np.zeros(n_features, dtype=bool)\n",
    "    idx = np.random.choice(n_features, size=k, replace=False)\n",
    "    mask[idx] = True\n",
    "    return mask\n",
    "\n",
    "def _masked_cosine(a, b, feat_mask: np.ndarray) -> float:\n",
    "    \"\"\"Cosine similarity in the chosen subspace (1 x d sparse rows).\"\"\"\n",
    "    return float(cosine_similarity(a[:, feat_mask], b[:, feat_mask])[0, 0])\n",
    "\n",
    "def _pick_chunk(vecs: List) -> np.ndarray:\n",
    "    \"\"\"Uniformly sample one chunk vector for this account (varies across trials).\"\"\"\n",
    "    if not vecs: raise ValueError(\"Empty chunk list for account.\")\n",
    "    return vecs[np.random.randint(len(vecs))]\n",
    "\n",
    "def impostors_score_chunks(\n",
    "    A_chunks: List, B_chunks: List, background_accounts: List[List],\n",
    "    n_features: int, n_trials: int = 400, feat_frac: float = 0.5, bg_accounts_per_trial: int = 25\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Fraction of trials where sim(A,B) > max sim(A, any sampled background account),\n",
    "    using a fresh random feature subspace each trial.\n",
    "    \"\"\"\n",
    "    poolN = len(background_accounts)\n",
    "    if poolN == 0:\n",
    "        return 0.0\n",
    "    wins = 0\n",
    "    k = min(bg_accounts_per_trial, poolN)\n",
    "    for _ in range(n_trials):\n",
    "        mask = _random_feature_mask(n_features, feat_frac)\n",
    "        vA = _pick_chunk(A_chunks)\n",
    "        vB = _pick_chunk(B_chunks)\n",
    "        sim_AB = _masked_cosine(vA, vB, mask)\n",
    "        idxs = np.random.choice(poolN, size=k, replace=False)\n",
    "        sims_AI = [_masked_cosine(vA, _pick_chunk(background_accounts[j]), mask) for j in idxs]\n",
    "        if sim_AB > max(sims_AI, default=-1.0):\n",
    "            wins += 1\n",
    "    return wins / n_trials\n",
    "\n",
    "def symmetric_impostors_score(\n",
    "    A_chunks: List, B_chunks: List, background_accounts: List[List],\n",
    "    n_features: int, n_trials: int = 400, feat_frac: float = 0.5, bg_accounts_per_trial: int = 25\n",
    ") -> float:\n",
    "    \"\"\"Average of A→B vs background and B→A vs background.\"\"\"\n",
    "    s1 = impostors_score_chunks(A_chunks, B_chunks, background_accounts,\n",
    "                                n_features, n_trials, feat_frac, bg_accounts_per_trial)\n",
    "    s2 = impostors_score_chunks(B_chunks, A_chunks, background_accounts,\n",
    "                                n_features, n_trials, feat_frac, bg_accounts_per_trial)\n",
    "    return 0.5 * (s1 + s2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a281d8",
   "metadata": {},
   "source": [
    "## Build all-pairs similarity (leave-two-out background)\n",
    "\n",
    "We treat *all* accounts uniformly. For each pair (A,B),\n",
    "the background is \"all other accounts\" (leave-two-out).\n",
    "This removes the need for a separate impostor list and keeps the setting fully unsupervised."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2dd4dc56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_similarity_matrix(\n",
    "    ids: List[str],\n",
    "    account_chunk_vectors: Dict[str, List],  # id -> [chunk vectors]\n",
    "    n_features: int,\n",
    "    n_trials: int = 800, feat_frac: float = 0.45, bg_accounts_per_trial: int = 40, seed: int = 42\n",
    ") -> np.ndarray:\n",
    "    np.random.seed(seed)\n",
    "    n = len(ids)\n",
    "    S = np.eye(n, dtype=float)  # S[i,i] = 1 by convention\n",
    "\n",
    "    # Pre-cache the chunk lists\n",
    "    chunks = {bid: account_chunk_vectors[bid] for bid in ids}\n",
    "\n",
    "    for i, j in itertools.combinations(range(n), 2):\n",
    "        A_id, B_id = ids[i], ids[j]\n",
    "        A_chunks, B_chunks = chunks[A_id], chunks[B_id]\n",
    "\n",
    "        # Background = all other accounts (leave two out)\n",
    "        background = [chunks[k] for k in ids if k not in (A_id, B_id)]\n",
    "\n",
    "        s = symmetric_impostors_score(\n",
    "            A_chunks, B_chunks, background, n_features,\n",
    "            n_trials=n_trials, feat_frac=feat_frac, bg_accounts_per_trial=bg_accounts_per_trial\n",
    "        )\n",
    "        S[i, j] = S[j, i] = s\n",
    "    return S\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fb2c18",
   "metadata": {},
   "source": [
    "## Community detection (unknown K) via Louvain\n",
    "Turn S into a graph (edge if similarity >= tau) and find communities that maximize modularity.\n",
    "We sweep tau and keep the partition with the best modularity Q (NetworkX provides both).\n",
    "`next implementation: Leiden`\n",
    "Note: Leiden improves Louvain (connectedness & quality); consider it for larger graphs. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a762411e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_with_louvain(S: np.ndarray, ids: List[str], tau: float = 0.65, resolution: float = 1.0, seed: int = 42):\n",
    "    \"\"\"\n",
    "    Threshold S at tau, build weighted graph, detect communities with Louvain.\n",
    "    Returns (clusters_dict, modularity_Q).\n",
    "    \"\"\"\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(ids)\n",
    "    for i in range(len(ids)):\n",
    "        for j in range(i+1, len(ids)):\n",
    "            w = float(S[i, j])\n",
    "            if w >= tau:\n",
    "                G.add_edge(ids[i], ids[j], weight=w)\n",
    "\n",
    "    if G.number_of_edges() == 0:\n",
    "        return {}, 0.0\n",
    "\n",
    "    # Louvain communities (greedy modularity optimization) — returns sets of nodes\n",
    "    comms = louvain_communities(G, weight=\"weight\", resolution=resolution, seed=seed)  \n",
    "\n",
    "    # Modularity score Q = quality of the partition (higher = clearer community structure)\n",
    "    from networkx.algorithms.community import modularity\n",
    "    Q = modularity(G, comms, weight=\"weight\") \n",
    "\n",
    "    clusters = {i: sorted(list(c)) for i, c in enumerate(comms)}\n",
    "    return clusters, Q\n",
    "\n",
    "def sweep_tau_for_best_modularity(S: np.ndarray, ids: List[str], taus=np.linspace(0.58, 0.76, 5), resolution: float = 1.0):\n",
    "    \"\"\"\n",
    "    Try several thresholds and keep the partition with highest modularity Q.\n",
    "    This removes the need to guess a single tau value up front.\n",
    "    \"\"\"\n",
    "    best = {\"tau\": None, \"Q\": -1, \"clusters\": None}\n",
    "    for tau in taus:\n",
    "        clusters, Q = cluster_with_louvain(S, ids, tau=tau, resolution=resolution)\n",
    "        if Q is not None and Q > best[\"Q\"]:\n",
    "            best = {\"tau\": tau, \"Q\": Q, \"clusters\": clusters}\n",
    "    return best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87f724b2",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a8bc2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[fit_vectorizer] n_docs=5 chunks_per_account={'acc_A': 1, 'acc_B': 1, 'acc_C': 1, 'acc_D': 1, 'acc_E': 1}  min_df=1  max_df=0.9\n",
      "IDs: ['acc_A', 'acc_B', 'acc_C', 'acc_D', 'acc_E']\n",
      "Similarity matrix (rounded):\n",
      " [[1.    0.    0.    1.    0.001]\n",
      " [0.    1.    0.734 0.    0.026]\n",
      " [0.    0.734 1.    0.    0.732]\n",
      " [1.    0.    0.    1.    0.001]\n",
      " [0.001 0.026 0.732 0.001 1.   ]]\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "\n",
    "vec, account_chunk_vectors, n_features = fit_vectorizer(\n",
    "    accounts, ngram_range=(3,5), min_df=None, max_df=None, sublinear_tf=True\n",
    ")\n",
    "\n",
    "ids_to_cluster = list(accounts.keys())\n",
    "S = build_similarity_matrix(\n",
    "    ids_to_cluster, account_chunk_vectors, n_features,\n",
    "    n_trials=900, feat_frac=0.45, bg_accounts_per_trial=50, seed=42\n",
    ")\n",
    "\n",
    "print(\"IDs:\", ids_to_cluster)\n",
    "print(\"Similarity matrix (rounded):\\n\", np.round(S, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6d71c541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Louvain] best_tau=0.58  modularity(Q)=0.482\n",
      "  Botnet 0: ['acc_A', 'acc_D']\n",
      "  Botnet 1: ['acc_B', 'acc_C', 'acc_E']\n"
     ]
    }
   ],
   "source": [
    "best = sweep_tau_for_best_modularity(S, ids_to_cluster, taus=np.linspace(0.58, 0.76, 5), resolution=1.0)\n",
    "print(f\"\\n[Louvain] best_tau={best['tau']:.2f}  modularity(Q)={best['Q']:.3f}\")\n",
    "for cid, members in (best[\"clusters\"] or {}).items():\n",
    "    print(f\"  Botnet {cid}: {members}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
